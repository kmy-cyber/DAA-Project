\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}

\geometry{a4paper, left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\begin{document}

\begin{titlepage}
    \centering

    \vspace*{-1cm}
    \includegraphics[width=4cm]{matcom.jpeg}
    \vspace{1cm}

    {\Large \textbf{Universidad de La Habana}}\\[0.3cm]
    {\large \textbf{Facultad de Matemática y Computación}}\\[1cm]

    {\large Asignatura: Diseño y Análisis de Algoritmos}\\[2cm]

    {\LARGE \textbf{Conectando la UH}}\\[0.8cm]

    {\Large \textbf{Integrantes}}\\[0.6cm]
    {\large Jabel Resendiz Aguirre}\\
    {\large Noel Pérez Calvo}\\
    {\large Arianne Camila Palancar Ochando }\\[1.7cm]

    {\large Carrera: Ciencia de la Computación}\\[2cm]

    \vfill
    {\large \today}
\end{titlepage}


\tableofcontents
\newpage

\begin{center}
\LARGE \textbf{Problema}
\end{center}
\addcontentsline{toc}{section}{Problema}

La Universidad de La Habana, en su constante búsqueda de la excelencia académica y la innovación, 
se ha embarcado en un proyecto crucial para modernizar y expandir su infraestructura de red. Nuestro 
objetivo es dotar a todas nuestras facultades, centros de investigación y edificios administrativos con
conectividad de fibra óptica de alta velocidad. Para este fin, contamos con el valioso apoyo técnico y
logístico de ETECSA (Empresa de Telecomunicaciones de Cuba S.A.).

Nos enfrentamos a un desafío de diseño de red que requiere una solución óptima. Necesitamos 
interconectar todos los edificios principales de la UH con fibra óptica, creando una red robusta 
y eficiente. Cada posible conexión de fibra entre dos edificios tiene un costo de instalación asociado,
que incluye desde los permisos internos y la mano de obra especializada de ETECSA hasta los materiales
y las obras civiles necesarias.

Sin embargo, ETECSA ha establecido una restricción técnica fundamental que debemos respetar:  

En cada edificio, la conexión de la fibra óptica se gestionará a través de un equipo de red central 
(un router o switch principal) que ellos nos proporcionan. Estos equipos tienen una capacidad limitada 
de puertos. Esto significa que un equipo en un edificio específico solo puede manejar un número máximo 
de conexiones de fibra óptica directas a otros edificios. Exceder este límite implicaría la necesidad 
de instalar equipos adicionales mucho más caros y complejos, o la implementación de soluciones de red 
alternativas que ETECSA no puede garantizar o que dispararían drásticamente el presupuesto del proyecto.

Nuestro objetivo principal es diseñar la red de fibra óptica que conecte todos nuestros edificios 
principales de la manera más económica posible. Esto implica seleccionar las rutas de fibra de tal 
forma que:

\begin{enumerate}
	\item Todos los edificios estén interconectados a la red principal de la universidad, sin crear bucles 
innecesarios (buscamos una estructura de red eficiente).
	\item Ningún equipo de red en ningún edificio exceda su capacidad máxima de conexiones directas 
(es decir, el número de cables de fibra que llegan o salen de un edificio no puede superar el límite 
de puertos del equipo de ETECSA).
	\item El costo total de instalación de toda la red sea el mínimo posible.
\end{enumerate}

Una planificación subóptima podría resultar en un sobrecosto significativo para la universidad, la 
necesidad de adquirir hardware de red adicional no previsto, o en una red ineficiente que no cumpla 
con las especificaciones técnicas y presupuestarias acordadas con ETECSA.

\clearpage
\begin{center}
\LARGE \textbf{Formalización del Problema}
\end{center}
\addcontentsline{toc}{section}{Fase 1: Formalización del Problema}

El objetivo es diseñar una red de fibra óptica que conecte todos los edificios 
principales de la Universidad de La Habana mediante enlaces posibles provistos por ETECSA. 
Cada enlace tiene un costo de instalación y cada edificio posee un límite máximo 
de puertos disponibles. Se requiere encontrar una configuración de conexiones que:

\begin{itemize}
	\item conecte todos los edificios,
	\item respete los límites de puertos por edificio,
	\item minimice el costo total de instalación.
\end{itemize}

\subsection*{Notaci\'on}
\addcontentsline{toc}{subsection}{Notaci\'on}

En el lenguaje matemático, podemos definir la estructura del problema como un grafo simple y no dirigido $G=(V,E)$, donde:

\begin{itemize}
	\item Un conjunto de edificios:
	\[
	V = \{v_1, v_2, \ldots, v_n\}.
	\]
	
	\item Un conjunto de posibles enlaces de fibra óptica: $E=(e_1,e_2,...,e_m)$
	\[
	E \subseteq \{\{u,v\} \mid u,v \in V,\; u \neq v\}.
	\]
	
	\item Un costo de instalación para cada enlace:
	\[
	c : E \rightarrow \mathbb{R}_{>0}.
	\]
	
	\item Un límite de puertos (grado máximo permitido) en cada edificio:
	\[
	d : V \rightarrow \mathbb{Z}_{\ge 1}.
	\]
\end{itemize}

\subsection*{Restricciones del Problema}
\addcontentsline{toc}{subsection}{Restricciones del Problema}

Sea un subgrafo de G como \(T=(V^T,E^T)\) debe satisfacer que:

\begin{enumerate}

	\item \textbf{Conectividad:}
	\[
	T \text{ es conexo } \rightarrow V^T = V
	\]
	
	\item \textbf{Estructura de árbol:}
	\[
	T \text{ no contiene ciclos} \rightarrow |E^T| = |V| - 1.
	\]
	
	\item \textbf{Límite de puertos por edificio:}
	\[
	\deg_{T}(v) \le d(v), \qquad \forall v \in V.
	\]
\end{enumerate}

donde se define $deg_{T}(v)$ como el grado del v\'ertice $v$ (tambi\'en llamado cardinalidad de su vecindad) en el grafo T.
Se desea conseguir el subgrafo $T$ tal que minimiza el costo de la suma de los pesos de sus aristas, es decir:

\begin{enumerate}
	\item \textbf{Funci\'on objetivo:}
	\[
	\min_{T \subseteq E} \sum_{e \in T} c(e)
	\]
\end{enumerate}

sujeto a las restricciones anteriores.

\newpage
\begin{center}
\LARGE \textbf{Análisis de Complejidad Computacional}
\end{center}
\addcontentsline{toc}{section}{Fase 2: Análisis de Complejidad Computacional}

En esta fase se determina la dificultad computacional del problema formalizado en la Fase~1.
Demostraremos que la versión de decisión del problema es NP-completa, y por lo tanto, 
la versión de optimización es NP-dura.

\subsection*{Versión de Decisión del Problema}

Dado un grafo $G=(V,E)$, costos $c(e)$, límites de grado $d(v)$ y un valor $K$, 
definimos la siguiente pregunta:

\[
\text{¿Existe un subconjunto } T \subseteq E \text{ tal que }
\begin{cases}
	G_T = (V,T) \text{ es un árbol}, \\
	\deg_{G_T}(v) \le d(v) \;\; \forall v \in V, \\
	\sum_{e \in T} c(e) \le K?
\end{cases}
\]

\subsection*{Pertenencia a NP}

Dado un conjunto de aristas $T$, quiere verificarse que se puede comprobar en tiempo polinomial, 
lo cual se cumplirá si se cumplen las siguientes condiciones:

\begin{itemize}
	\item $G_T$ es conexo (mediante un recorrido BFS/DFS),
	\item $|T| = |V|-1$, es decir el subgrafo $G_T$ forma un árbol,
	\item $\deg_{G_T}(v) \le d(v)$ para todo $v \in V$,
	\item $\sum_{e \in T} c(e) \le K$.
\end{itemize}

Por lo tanto, el problema pertenece a NP.

\subsection*{Demostración de NP-completitud}

Para demostrar que dicho problema es NP-completo, se presenta una 
reducción en tiempo polinomial desde el problema \textsc{Hamiltonian Path} en 
grafos no dirigidos, el cual es NP-completo.

\subsubsection*{Problema de Partida: Hamiltonian Path}

Dado un grafo no dirigido $G=(V,E)$, el problema \textsc{Hamiltonian Path} 
pregunta si existe un camino que visite todos los vértices exactamente una vez.

\subsubsection*{Construcción de la Reducción}

A partir de una instancia de \textsc{Hamiltonian Path}, construimos una instancia 
del problema de la siguiente manera:

\begin{itemize}
	\item Se toma el mismo grafo $G=(V,E)$.
	\item Para cada arista $e \in E$, se asigna un costo $c(e)=1$.
	\item Se fija un límite de grado uniforme:
	\[
	d(v) = 2 \quad \forall v \in V.
	\]
	\item Se establece el umbral de costo:
	\[
	K = |V| - 1.
	\]
\end{itemize}

La construcción es claramente polinomial.

\subsubsection*{Correctitud de la Reducción}

\paragraph{($\Rightarrow$)}  
Si $G$ posee un camino hamiltoniano, dicho camino contiene $|V|-1$ aristas, 
es conexo, acíclico y cada vértice tiene grado a lo sumo $2$.  
Por lo tanto, constituye un conjunto $T$ válido para el problema
con costo total $|V|-1 \le K$.

\paragraph{($\Leftarrow$)}  
Si existe un conjunto $T$ que satisface las restricciones del problema,
entonces $T$ es un árbol con $|V|-1$ aristas y $\deg_{G_T}(v) \le 2$ para todo $v$.  
La única estructura de árbol donde todos los grados son a lo sumo $2$ es un camino.  
Por lo tanto, $T$ es un camino hamiltoniano del grafo original.

\subsubsection*{Conclusión}

La existencia de un $T$ válido para \textsc{DC-MST-Decision} es equivalente 
a la existencia de un camino hamiltoniano en $G$.  
Dado que la reducción es polinomial y \textsc{Hamiltonian Path} es NP-completo, se concluye:

\[
\textsc{DC-MST-Decision} \text{ es NP-completo}.
\]

\subsection*{Consecuencia}

Dado que la versión de decisión es NP-completa, la versión de optimización 
(\textit{Degree-Constrained Minimum Spanning Tree}) es NP-dura:

\[
\text{DC-MST es NP-dura}.
\]

Esto implica que no se conoce un algoritmo polinomial que resuelva el problema de forma
óptima para instancias generales, salvo que $\mathrm{P} = \mathrm{NP}$.

\newpage
\begin{center}
\LARGE \textbf{Diseño de Soluciones Algorítmicas}
\end{center}
\addcontentsline{toc}{section}{Fase 3: Diseño de Soluciones Algorítmicas}

El problema de construir un árbol expansivo con restricciones de grado
(DC-MST) es NP-difícil, lo que implica que no existe un algoritmo
polinomial conocido capaz de garantizar la solución óptima para
instancias generales. Por ello, en esta sección se exploran distintas
estrategias algorítmicas para abordar el problema, combinando
métodos exactos y heurísticos.

Presentaremos los algoritmos propuestos de manera estructurada,
incluyendo:

\begin{itemize}
    \item Una descripción de la idea principal de cada algoritmo.
    \item El pseudocódigo correspondiente para su implementación.
    \item El análisis de complejidad computacional.
\end{itemize}

Esto permitirá evaluar las ventajas y limitaciones de cada enfoque,
y proporcionará una base sólida para la implementación experimental
y la comparación de resultados en la Fase 4.

\subsection*{Algoritmo de Fuerza Bruta: Exploración Exhaustiva del Espacio de Árboles Expansivos}
\addcontentsline{toc}{subsection}{Algoritmo de Fuerza Bruta: Exploración Exhaustiva del Espacio de Árboles Expansivos}

Dado que el problema de encontrar un árbol expansivo de costo mínimo con
restricciones de grado es NP-difícil, una primera aproximación consiste en
analizar exhaustivamente todas las posibles soluciones y seleccionar aquella
que cumple las restricciones y minimiza el costo total.

Un candidato a solución válida debe ser un \textbf{árbol expansivo}, es decir,
un subgrafo conexo y acíclico que contiene todos los vértices del grafo y
exactamente $|V|-1$ aristas. El algoritmo de fuerza bruta consiste en enumerar
todos los subconjuntos de aristas del grafo y verificar cuáles de ellos cumplen
las condiciones necesarias para ser un árbol expansivo factible.

\subsubsection*{Idea del Algoritmo}

Sea $G=(V,E)$ un grafo no dirigido con $|V|=n$ vértices y $|E|=m$ aristas.
El procedimiento seguido es el siguiente:

\begin{enumerate}
    \item Enumerar todos los subconjuntos $S \subseteq E$.
    \item Descartar aquellos subconjuntos que no contengan exactamente $n-1$ aristas.
    \item Para cada subconjunto candidato:
    \begin{itemize}
        \item Verificar que el subgrafo inducido sea conexo y acíclico, utilizando
        una estructura de datos \emph{Disjoint Set Union} (DSU).
        \item Comprobar que el grado de cada vértice no exceda su límite máximo
        permitido.
        \item Calcular el costo total de las aristas seleccionadas.
    \end{itemize}
    \item Conservar el subconjunto válido cuyo costo total sea mínimo.
\end{enumerate}

Este algoritmo garantiza encontrar la solución óptima, aunque a costa de un
tiempo de ejecución exponencial.

\subsubsection*{Pseudocódigo}

\begin{lstlisting}[language=]
BruteForce-DCMST(G):
    bestCost <- +infinity
    bestTree <- empty

    for each subset S of E:
        if |S| != |V| - 1:
            continue

        initialize DSU with |V| elements
        deg[v] <- 0 for all v in V
        cost <- 0
        valid <- true

        for each edge (u,v) in S:
            if deg[u] + 1 > d(u) or deg[v] + 1 > d(v):
                valid <- false
                break

            deg[u] <- deg[u] + 1
            deg[v] <- deg[v] + 1

            if DSU.find(u) == DSU.find(v):
                valid <- false
                break

            DSU.union(u,v)
            cost <- cost + c(u,v)

        if valid and cost < bestCost:
            bestCost <- cost
            bestTree <- S

    return bestCost, bestTree
\end{lstlisting}

\subsubsection*{Análisis de Complejidad}

Sea $n = |V|$ el número de vértices y $m = |E|$ el número de aristas del grafo.

\begin{itemize}
    \item El algoritmo enumera todos los subconjuntos posibles de aristas, lo cual
    implica $2^m$ iteraciones.
    \item Para cada subconjunto candidato con $n-1$ aristas, se realizan:
    \begin{itemize}
        \item Operaciones de unión y búsqueda en la estructura DSU, con costo
        $O(n \alpha(n)) \approx O(n)$.
        \item Verificación de los límites de grado y cálculo del costo total,
        ambos en $O(n)$.
    \end{itemize}
\end{itemize}

Por tanto, la complejidad temporal total del algoritmo es:

\[
\boxed{
O(2^m \cdot n)
}
\]

En el peor caso, cuando el grafo es denso y $m = O(n^2)$, la complejidad crece
como $O(2^{n^2})$, lo cual hace que este enfoque sea impracticable para
instancias de tamaño moderado o grande.

\subsubsection*{Conclusión}

El algoritmo de fuerza bruta permite obtener la solución óptima del problema de
diseño de la red de fibra óptica bajo restricciones de grado. Sin embargo, su
costo computacional exponencial limita su uso a instancias muy pequeñas. Esto
justifica la necesidad de desarrollar algoritmos heurísticos o aproximados,
los cuales se abordan en las siguientes secciones.

\subsection*{Algoritmo Heurístico: Reducción del Árbol de Oportunidades}
\addcontentsline{toc}{subsection}{Algoritmo Heurístico: Reducción del Árbol de Oportunidades}

Con el objetivo de disminuir el tamaño del espacio de búsqueda y la dificultad
computacional del problema, se introducen una serie de propiedades estructurales
del \textbf{árbol expansivo mínimo con restricciones de grado} que permiten
reducir el grafo original sin perder optimalidad.

Sea $T^*$ un árbol expansivo mínimo con restricciones de grado del grafo
$G = (V,E)$.

\subsubsection*{Propiedades Estructurales}

\paragraph{Lema 1.}
Sea $v \in V$ un vértice hoja, es decir, un vértice con grado uno en el grafo
original $G$. Entonces, la única arista incidente a $v$ debe pertenecer a $T^*$.

\paragraph{Demostración.}
Dado que $T^*$ es un grafo conexo que contiene todos los vértices de $G$, el vértice
$v$ debe estar conectado al resto del árbol mediante su única arista incidente.
Excluir dicha arista implicaría que $v$ quedaría aislado, contradiciendo la
conectividad de $T^*$. Por tanto, toda arista incidente a un vértice colgante debe
incluirse necesariamente en $T^*$. \hfill $\square$

\vspace{0.2cm}

\paragraph{Lema 2.}
Sea $V_1 = \{ v_i \in V \mid d(v_i) = 1 \}$ el conjunto de vértices cuyo grado máximo
permitido es uno, y sea
\[
E_1 = \{ (v_i,v_j) \in E \mid v_i, v_j \in V_1 \}.
\]
Si $|V| > 2$, entonces ninguna arista de $E_1$ puede pertenecer a $T^*$.

\paragraph{Demostración.}
Supóngase que existe una arista $(v_i,v_j) \in E_1$ incluida en $T^*$. Como
$d(v_i)=d(v_j)=1$, ninguno de estos vértices puede conectarse con ningún otro
vértice adicional en $T^*$. Esto implica que el subgrafo resultante no puede ser
conexo cuando $|V|>2$, lo cual contradice la definición de árbol expansivo.
Por tanto, todas las aristas de $E_1$ deben ser excluidas de $T^*$. \hfill $\square$

\vspace{0.2cm}

\paragraph{Lema 3.}
Sea $v_k$ un vértice de grado dos en $G$, con vecinos $v_i$ y $v_j$. Si no existe
ningún camino entre $v_i$ y $v_j$ que no pase por $v_k$, entonces las aristas
$(v_k,v_i)$ y $(v_k,v_j)$ deben pertenecer a $T^*$.

\paragraph{Demostración.}
Supóngase que alguna de las aristas $(v_k,v_i)$ o $(v_k,v_j)$ no pertenece a $T^*$.
Dado que no existe un camino alternativo entre $v_i$ y $v_j$ que evite el vértice
$v_k$, se deduce que $T^*$ no puede ser conexo, lo cual contradice su definición.
Por tanto, ambas aristas deben incluirse necesariamente en $T^*$. \hfill $\square$

---

\subsubsection*{Algoritmo de Reducción}

A partir de las propiedades anteriores, se define un procedimiento de reducción
que simplifica el grafo original antes de aplicar un algoritmo constructivo.

\begin{lstlisting}
Reduction_DCMST(G = (V,E), d):
    T* <- empty
    1. Eliminar todas las aristas que satisfacen el Lema 2.
    2. Identificar todos los v\'ertices colgantes (grado 1),
       agregar sus aristas incidentes a T* y eliminarlos del grafo.
    3. Identificar v\'ertices que satisfacen el Lema 3,
       agregar las aristas correspondientes a T* y eliminarlas del grafo.
    return G reducido, T*
\end{lstlisting}

El costo computacional de este algoritmo de reducción es:

\begin{itemize}
    \item Paso 1: $O(n^2)$,
    \item Paso 2: $O(n)$,
    \item Paso 3: $O(n^3)$.
\end{itemize}

Por tanto, la complejidad temporal total del algoritmo de reducción es:

\[
O(n^3)
\]

---

\subsubsection*{Construcción del Árbol con Restricciones de Grado}

Una vez reducido el grafo, se construye un árbol expansivo utilizando un algoritmo
greedy basado en el algoritmo clásico de Kruskal, adaptado para respetar las
restricciones de grado.

El algoritmo itera seleccionando aristas de menor costo que conecten componentes
distintas y que no violen los límites de grado de los vértices involucrados. Este
procedimiento continúa hasta obtener $|V|-1$ aristas.


\begin{lstlisting}
MAIN_DCMST(G = (V, E), w, d):
    V* <- V
    E* <- empty
    T* <- (V*, E*)

    1. Ejecutar REDUCTION_DCMST(G, d)
    
    2. E1 <- E

    3. while |E*| < |V| - 1 do
           seleccionar la arista de menor costo
           emin = (vk, vh) en E1
           
           eliminar emin de E1
           
           if vk y vh estan en componentes distintas de T*
              and deg_T*(vk) < d(vk)
              and deg_T*(vh) < d(vh) then
               
               E* <- E* U {emin}
               unir las componentes de vk y vh en T*
           end if
       end while

    4. Aplicar tecnicas de intercambio de aristas
       (1-opt y 2-opt) para mejorar la solucion

    return G* = (V*, E*)
\end{lstlisting}


---

\subsubsection*{Técnicas de Intercambio de Aristas}

Para mejorar la solución obtenida, se emplean técnicas de \emph{edge exchange}.

\paragraph{Intercambio 1-opt.}
Consiste en eliminar una arista del árbol y reemplazarla por una arista externa,
siempre que el resultado sea un árbol. Esta operación puede modificar los grados
de los vértices y, por tanto, debe verificarse nuevamente la restricción de grado.

\paragraph{Intercambio 2-opt.}
Consiste en reemplazar dos aristas del árbol por dos aristas externas, de forma
que el grado de cada vértice se conserve. Esta operación garantiza que la
restricción de grado siga siendo satisfecha y permite mejorar el costo total del
árbol.

---

\subsubsection*{Complejidad del Algoritmo Heurístico}

La complejidad temporal del algoritmo completo es la siguiente:

\begin{itemize}
    \item Reducción del grafo: $O(n^3)$.
    \item Construcción inicial del árbol (Kruskal modificado): $O(n)$.
    \item Intercambio 1-opt: $O(n)$.
    \item Intercambio 2-opt: $O(n^2)$.
\end{itemize}

Por tanto, la complejidad temporal total del algoritmo heurístico es:

\[
\boxed{O(n^3)}
\]

Este enfoque permite obtener soluciones de alta calidad en tiempo polinomial,
haciendo viable el tratamiento de instancias de tamaño considerable.

\subsection*{Heurística Lagrangiana para DC-MST}
\addcontentsline{toc}{subsection}{Heurística Lagrangiana para DC-MST}

El enfoque de relajación Lagrangiana constituye una técnica poderosa para 
obtener cotas inferiores de calidad y guiar la construcción de soluciones 
factibles para problemas de optimización combinatoria NP-duros. En el contexto
del DC-MST, se aplica esta técnica relajando las restricciones de grado 
mediante multiplicadores Lagrangianos, lo que transforma el problema original
en un problema de árbol generador mínimo (MST) clásico, resoluble en tiempo
polinomial.

\subsubsection*{Idea del Algoritmo}

La heurística Lagrangiana propuesta se compone de tres componentes principales
que trabajan de forma integrada:

\paragraph{1. Relajación Lagrangiana del DC-MST.}
Se relajan las restricciones de grado mediante multiplicadores Lagrangianos
$\lambda = (\lambda_1, \lambda_2, \ldots, \lambda_n) \in \mathbb{R}_+^{|V|}$,
donde $\lambda_i$ penaliza las violaciones de la restricción de grado en el
vértice $i \in V$.

El subproblema de relajación Lagrangiana (LRS) resultante es:

\[
z(\lambda) = \min \left\{ \sum_{e=[i,j] \in E} (c_e + \lambda_i + \lambda_j) x_e 
- \sum_{i \in V} \lambda_i d_i : x \in R_0 \right\}
\]

donde $R_0$ representa el conjunto de vectores de incidencia de árboles 
generadores de $G$. Este subproblema equivale a encontrar un MST con costos
modificados $\{c_e + \lambda_i + \lambda_j : e=[i,j] \in E\}$, resoluble 
eficientemente mediante algoritmos de Kruskal o Prim.

Para obtener la mejor cota inferior posible, se resuelve el problema dual
Lagrangiano:

\[
z(\lambda^*) = \max_{\lambda \geq 0} \left\{ z(\lambda) \right\}
\]

El vector óptimo $\lambda^*$ se obtiene mediante el \textbf{método del 
subgradiente}, que actualiza iterativamente los multiplicadores según:

\[
\lambda_i^{k+1} = \max\left\{0, \lambda_i^k + t^k s_i^k\right\}, 
\quad \forall i \in V
\]

donde $s_i^k = \sum_{e \in \delta(i)} x_e^k - d_i$ es el subgradiente
(violación de la restricción de grado en el vértice $i$), y el tamaño de
paso $t^k$ se calcula como:

\[
t^k = \frac{(1 + \alpha) z_{UB} - z(\lambda^k)}{\|s^k\|^2}
\]

siendo $z_{UB}$ una cota superior conocida y $\alpha$ un parámetro de 
control (típicamente $\alpha = 0.03$).

\paragraph{2. Construcción Heurística con Prevención de Infactibilidad.}
Se introduce el algoritmo \texttt{KRUSKALX}, una variante del algoritmo
clásico de Kruskal que incorpora un mecanismo de \emph{look-ahead} para
prevenir la generación de soluciones infactibles.

La idea fundamental es el concepto de \textbf{saturación}. Sea $T=(V_T, E_T)$
un subárbol. Se definen:

\begin{itemize}
    \item \textbf{Grado del árbol:} $\delta(T) = \sum_{i \in V_T} \sum_{e \in \delta(i) \cap E_T} 1$
    \item \textbf{Capacidad del árbol:} $d(T) = \sum_{i \in V_T} d_i$
\end{itemize}

Un árbol $T$ se considera \textbf{saturado} si $\delta(T) = d(T)$, es decir,
si ha alcanzado su capacidad máxima de aristas respetando las restricciones
de grado.

\textbf{Proposición (Condición de No Saturación):}
Sean $T_1$ y $T_2$ dos subárboles disjuntos no saturados. Si existe una arista
$e \in E$ tal que $T_3 = (V_1 \cup V_2, E_1 \cup E_2 \cup \{e\})$ es también
no saturado, entonces:

\[
\delta(T_3) = \delta(T_1) + \delta(T_2) + 2 < d(T_3)
\]

Esta propiedad garantiza que en grafos completos siempre es posible construir
una solución factible, incluso cuando algunos vértices tienen $d_i \in \{1,2\}$.

El algoritmo \texttt{KRUSKALX} ordena las aristas por costo creciente y las
considera secuencialmente. Una arista $e=[i,j]$ se añade al árbol en
construcción si y solo si:

\begin{enumerate}
    \item Conecta dos componentes distintas (no crea ciclos)
    \item No viola las restricciones de grado: $\deg(i) < d_i$ y $\deg(j) < d_j$
    \item La unión resultante no está saturada (excepto en la última arista)
\end{enumerate}

Para guiar la construcción hacia soluciones de buena calidad, se utilizan
\textbf{costos complementarios}. Si $x^k$ es la solución del LRS en la
iteración $k$, los costos se modifican como:

\[
\bar{c}_e = (1 - x_e^k) c_e, \quad \forall e \in E
\]

Esto hace que las aristas seleccionadas en la solución Lagrangiana sean
más atractivas para el algoritmo constructivo.

\paragraph{3. Problema Restringido.}
Para mejorar la eficiencia computacional, se define un \textbf{problema 
restringido} sobre un subconjunto de aristas $E' \subseteq E$. Dado el
ordenamiento de aristas $\{e_1, e_2, \ldots, e_m\}$ usado por 
\texttt{KRUSKALX} y siendo $k^*$ el índice de la última arista insertada,
se define:

\[
E' = \{e_1, e_2, \ldots, e_{m^*}\}, \quad \text{donde } m^* = \min\{2k^*, m\}
\]

El grafo restringido $G' = (V, E')$ contiene típicamente una fracción pequeña
de las aristas originales pero preserva soluciones de alta calidad. Trabajar
sobre $G'$ reduce significativamente el tiempo de cómputo permitiendo abordar
instancias con miles de vértices.

\paragraph{4. Procedimiento de Mejora Local.}
Una vez construida una solución factible $T$, se aplica un procedimiento de
mejora por intercambio de aristas. Para cada arista $e \in T$:

\begin{enumerate}
    \item Se elimina $e$ de $T$, generando dos subárboles $T_1$ y $T_2$
    \item Se busca la arista de menor costo $\bar{e} \in E \setminus T$ que 
          reconecte $T_1$ y $T_2$ sin violar restricciones de grado
    \item Si $c_{\bar{e}} < c_e$, se realiza el intercambio: 
          $T \leftarrow (T \setminus \{e\}) \cup \{\bar{e}\}$
\end{enumerate}

Este procedimiento tiene complejidad $O(|V| \cdot |E|)$ y se ejecuta sobre
el árbol obtenido en cada iteración del método del subgradiente.

\subsubsection*{Pseudocódigo}

\begin{lstlisting}
KRUSKALX(G, c, d):
    Ordenar aristas E = {e1, e2, ..., em} por costo creciente
    Inicializar deg[i] <- 0 para todo i en V
    Inicializar T <- (V, empty), k <- 1
    
    while |E(T)| < |V| - 1 and k <= m:
        ek <- [i, j]
        
        if deg[i] < d[i] and deg[j] < d[j]:
            if ek no forma ciclo en T:
                if |E(T)| = |V| - 2:
                    /* Ultima arista, agregar directamente */
                    E(T) <- E(T) U {ek}
                    deg[i] <- deg[i] + 1
                    deg[j] <- deg[j] + 1
                else:
                    /* Verificar condicion de no saturacion */
                    T1 <- componente de i en T
                    T2 <- componente de j en T
                    if delta(T1) + delta(T2) + 2 < d(T1) + d(T2):
                        E(T) <- E(T) U {ek}
                        deg[i] <- deg[i] + 1
                        deg[j] <- deg[j] + 1
                    end if
                end if
            end if
        end if
        k <- k + 1
    end while
    
    return T, k*
\end{lstlisting}

\vspace{0.3cm}

\begin{lstlisting}
IMPROVEMENT_PROCEDURE(T, G, c, d):
    E0 <- E(T)
    improved <- true
    
    while improved:
        improved <- false
        for each e = [i,j] in E0:
            Eliminar e de T, obteniendo T1 y T2
            
            /* Buscar mejor arista de reconexion */
            e_best <- null
            cost_best <- +infinito
            
            for each e' = [u,v] in E \ E(T):
                if u en V(T1) and v en V(T2) (o viceversa):
                    if deg_T(u) < d[u] or u in {i,j}:
                        if deg_T(v) < d[v] or v in {i,j}:
                            if c[e'] < cost_best:
                                e_best <- e'
                                cost_best <- c[e']
                            end if
                        end if
                    end if
                end if
            end for
            
            if e_best != null and cost_best < c[e]:
                E(T) <- (E(T) \ {e}) U {e_best}
                improved <- true
            else:
                Restaurar e en T
            end if
        end for
    end while
    
    return T
\end{lstlisting}

\vspace{0.3cm}

\begin{lstlisting}
LAGRANGIAN_HEURISTIC(G, c, d):
    /* Fase 1: Inicializacion */
    T_init <- KRUSKALX(G, c, d)
    E' <- primeras 2*k* aristas del ordenamiento
    G' <- (V, E')
    
    lambda[i] <- 0 para todo i en V
    z_LB <- -infinito
    z_UB <- costo(T_init)
    T_best <- T_init
    
    alpha <- 2.0
    iter <- 0
    max_iter <- 1000
    iter_sin_mejora <- 0
    
    /* Fase 2: Metodo del subgradiente */
    while iter < max_iter and z_UB - z_LB > 0.99:
        /* Resolver LRS con costos modificados */
        c_mod[e=[i,j]] <- c[e] + lambda[i] + lambda[j]
        T_LRS <- MST(G', c_mod)
        z_lambda <- costo(T_LRS) - sum(lambda[i] * d[i])
        
        if z_lambda > z_LB:
            z_LB <- z_lambda
            iter_sin_mejora <- 0
        else:
            iter_sin_mejora <- iter_sin_mejora + 1
        end if
        
        /* Si LRS es factible para DC-MST */
        if deg_T_LRS[i] <= d[i] para todo i en V:
            T_LRS <- IMPROVEMENT_PROCEDURE(T_LRS, G', c, d)
            if costo(T_LRS) < z_UB:
                z_UB <- costo(T_LRS)
                T_best <- T_LRS
            end if
        end if
        
        /* Construccion heuristica con costos complementarios */
        c_comp[e] <- (1 - x_LRS[e]) * c[e]
        T_heur <- KRUSKALX(G', c_comp, d)
        
        if T_heur es factible:
            T_heur <- IMPROVEMENT_PROCEDURE(T_heur, G', c, d)
            if costo(T_heur) < z_UB:
                z_UB <- costo(T_heur)
                T_best <- T_heur
            end if
        end if
        
        /* Actualizar multiplicadores Lagrangianos */
        s[i] <- deg_T_LRS[i] - d[i] para todo i en V
        t <- alpha * (z_UB - z_lambda) / ||s||^2
        
        for i = 1 to |V|:
            if s[i] > 0 or lambda[i] > 0:
                lambda[i] <- max(0, lambda[i] + t * s[i])
            end if
        end for
        
        /* Ajustar parametro alpha */
        if iter_sin_mejora > 30:
            alpha <- alpha * 0.5
            iter_sin_mejora <- 0
        end if
        
        iter <- iter + 1
    end while
    
    return T_best, z_LB, z_UB
\end{lstlisting}

\subsubsection*{Análisis de Complejidad}

Sea $n = |V|$, $m = |E|$ y $m' = |E'|$ el tamaño del problema restringido.

\paragraph{Complejidad de KRUSKALX:}
\begin{itemize}
    \item Ordenamiento de aristas: $O(m \log m)$
    \item Construcción del árbol con verificación de saturación: $O(m \alpha(n))$
    \item Complejidad total: $O(m \log m + m \alpha(n)) = O(m \log m)$
\end{itemize}

\paragraph{Complejidad del Procedimiento de Mejora:}
\begin{itemize}
    \item Para cada arista en $T$: $O(n)$ iteraciones
    \item Búsqueda de mejor reconexión: $O(m')$
    \item Complejidad por iteración: $O(n \cdot m')$
    \item En el peor caso (múltiples pasadas): $O(n^2 \cdot m')$
\end{itemize}

\paragraph{Complejidad de una Iteración del Método del Subgradiente:}
\begin{itemize}
    \item Resolver LRS (MST): $O(m' \log m')$
    \item Ejecutar KRUSKALX: $O(m' \log m')$
    \item Procedimiento de mejora: $O(n^2 \cdot m')$
    \item Actualización de multiplicadores: $O(n)$
    \item Total por iteración: $O(n^2 \cdot m' + m' \log m')$
\end{itemize}

\paragraph{Complejidad Total del Algoritmo:}

Sea $K$ el número de iteraciones del método del subgradiente (típicamente 
$K = O(1000)$ en la práctica). La complejidad total es:

\[
\boxed{O\left(K \cdot (n^2 \cdot m' + m' \log m')\right)}
\]

Para grafos completos, $m = \Theta(n^2)$ y típicamente $m' = O(n \log n)$
debido al problema restringido. En este caso:

\[
O(K \cdot n^3 \log n)
\]

Esta complejidad es manejable para instancias con miles de vértices, como
demuestran los experimentos computacionales reportados en la literatura,
donde se resuelven instancias con hasta 2000 vértices.

\paragraph{Ventajas del Enfoque Lagrangiano:}

\begin{enumerate}
    \item \textbf{Cotas de calidad:} Proporciona cotas inferiores que permiten
    evaluar la calidad de las soluciones heurísticas.
    
    \item \textbf{Guiado inteligente:} La información dual guía la construcción
    de soluciones factibles hacia regiones prometedoras del espacio de búsqueda.
    
    \item \textbf{Prueba de optimalidad:} Cuando $z_{UB} - z_{LB} < 1$ (para
    costos enteros), se certifica que la solución es óptima.
    
    \item \textbf{Escalabilidad:} El problema restringido permite trabajar con
    grafos grandes manteniendo la calidad de las soluciones.
    
    \item \textbf{Robustez:} Garantiza encontrar soluciones factibles en grafos
    completos, incluso con restricciones de grado muy ajustadas ($d_i \in \{1,2\}$).
\end{enumerate}

\newpage
\begin{center}
\LARGE \textbf{Implementación y Análisis Experimental}
\end{center}
\addcontentsline{toc}{section}{Fase 4: Implementación y Análisis Experimental}

En esta fase se presenta la implementación práctica de los tres algoritmos propuestos y se realiza
un análisis experimental exhaustivo para evaluar su comportamiento en términos de calidad de solución,
tiempo de ejecución y escalabilidad.

\subsection*{Metodología de Experimentación}
\addcontentsline{toc}{subsection}{Metodología de Experimentación}

Los experimentos se realizaron bajo las siguientes condiciones:

\begin{itemize}
    \item \textbf{Lenguaje:} C++ (estándar C++11)
    \item \textbf{Compilador:} g++ con optimizaciones -O2
    \item \textbf{Estructura de datos:} Disjoint Set Union (DSU) con path compression
    \item \textbf{Generación de instancias:} Grafos completos aleatorios con:
    \begin{itemize}
        \item Costos de aristas: enteros aleatorios en el rango [1, 100]
        \item Restricciones de grado: aleatorias respetando $d(v) \geq 1$ y $\sum d(v) \geq 2(n-1)$
    \end{itemize}
    \item \textbf{Timeout:} 5 segundos (5000 ms) por instancia
    \item \textbf{Métricas evaluadas:} Tiempo de ejecución, calidad de solución, escalabilidad
\end{itemize}

\subsection*{Tamaño Máximo de Instancia para Fuerza Bruta}
\addcontentsline{toc}{subsection}{Tamaño Máximo de Instancia para Fuerza Bruta}

El algoritmo de fuerza bruta, debido a su complejidad exponencial $O(2^m \cdot n)$, presenta
limitaciones severas en términos de escalabilidad. Los experimentos revelan que:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Tamaño (n)} & \textbf{Tiempo (ms)} & \textbf{Estado} \\
\hline
5 & 5.084 & OK \\
10 & 8.090 & OK \\
15 & 6.562 & OK \\
20 & 61890.611 & TLE \\
\hline
\end{tabular}
\caption{Escalabilidad del algoritmo de Fuerza Bruta}
\end{table}

\paragraph{Conclusión:} El límite práctico de viabilidad para el algoritmo de fuerza bruta
se encuentra en $\mathbf{n \approx 15}$ vértices. Para $n=20$, el tiempo de ejecución excede
ampliamente los 5 segundos de timeout establecido, alcanzando 61.89 segundos. Este comportamiento
confirma la naturaleza exponencial del algoritmo y justifica plenamente la necesidad de
algoritmos heurísticos para instancias de tamaño real.

El crecimiento explosivo del tiempo de ejecución entre $n=15$ y $n=20$ ($\sim$9400x) evidencia
que cada incremento en el tamaño del grafo multiplica el espacio de búsqueda de forma
exponencial. Para grafos completos, pasar de 15 a 20 nodos significa evaluar $2^{190}$ vs
$2^{105}$ subconjuntos potenciales de aristas.

\subsection*{Análisis Empírico Comparativo}
\addcontentsline{toc}{subsection}{Análisis Empírico Comparativo}

A continuación se presenta un análisis comparativo de los tres enfoques implementados en
términos de tiempo de ejecución y escalabilidad.

\subsubsection*{Comparación de Tiempos de Ejecución}

Para instancias pequeñas donde todos los algoritmos son viables ($n \leq 15$), se observa:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{n} & \textbf{Brute Force (ms)} & \textbf{DCMST (ms)} & \textbf{Lagrange (ms)} \\
\hline
5 & 5.084 & 3.582 & 2.800 \\
10 & 8.090 & 3.515 & 3.545 \\
15 & 6.562 & 4.421 & 5.658 \\
\hline
\end{tabular}
\caption{Comparación de tiempos en instancias pequeñas}
\end{table}

\paragraph{Observaciones:}
\begin{itemize}
    \item Para $n \leq 15$, todos los algoritmos presentan tiempos comparables (milisegundos).
    \item Las heurísticas (DCMST y Lagrange) son consistentemente más rápidas que Fuerza Bruta.
    \item Lagrange muestra ventaja en instancias muy pequeñas, pero DCMST es más consistente.
\end{itemize}

\paragraph{Nota sobre Limitaciones del Análisis Experimental:}

El análisis experimental implementado se centra exclusivamente en la medición de tiempos
de ejecución y escalabilidad de los algoritmos. Una \textbf{limitación importante} es que
no se implementó la comparación sistemática de la calidad de las soluciones obtenidas
por las heurísticas versus la solución óptima garantizada por Fuerza Bruta.

Un análisis completo requeriría:
\begin{itemize}
    \item Registrar y comparar los costos de las soluciones encontradas por cada algoritmo
    \item Calcular gaps de optimalidad en instancias pequeñas donde Fuerza Bruta es viable
    \item Cuantificar empíricamente la frecuencia con que las heurísticas encuentran el óptimo
\end{itemize}

Esta comparación de calidad queda como trabajo pendiente de implementación experimental.

\subsubsection*{Escalabilidad de las Heurísticas}

Para instancias de tamaño medio y grande, comparamos DCMST y Lagrange:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{n} & \textbf{DCMST (ms)} & \textbf{Estado} & \textbf{Lagrange (ms)} & \textbf{Estado} \\
\hline
20 & 5.497 & OK & 3.771 & OK \\
30 & 4.819 & OK & 5.239 & OK \\
60 & 6.264 & OK & 8.490 & OK \\
120 & 21.752 & OK & 34.431 & OK \\
240 & 170.350 & OK & 132.808 & OK \\
480 & 435.045 & OK & 862.332 & OK \\
960 & 3758.735 & OK & 6752.041 & TLE \\
1920 & 32250.960 & TLE & --- & --- \\
\hline
\end{tabular}
\caption{Escalabilidad comparativa de las heurísticas}
\end{table}

\paragraph{Análisis de Resultados:}

\begin{enumerate}
    \item \textbf{DCMST (Reducción del Árbol de Oportunidades):}
    \begin{itemize}
        \item Escala exitosamente hasta $n=960$ con tiempo $\sim$3.76 segundos.
        \item Para $n=1920$, excede el timeout (32.25 segundos).
        \item Complejidad $O(n^3)$ observable: duplicar $n$ multiplica el tiempo $\sim$8x.
        \item Es el algoritmo más escalable de los tres implementados.
    \end{itemize}

    \item \textbf{Lagrange (Relajación Lagrangiana):}
    \begin{itemize}
        \item Funciona eficientemente hasta $n=480$ ($\sim$862ms).
        \item Para $n=960$, excede el timeout (6.75 segundos).
        \item A pesar de su sofisticación teórica, el overhead de las iteraciones del
              subgradiente y las operaciones de mejora limitan su escalabilidad.
        \item El problema restringido ($E'$) crece significativamente en grafos grandes.
    \end{itemize}
\end{enumerate}

\subsubsection*{Análisis de Crecimiento Temporal}

Calculando las tasas de crecimiento observadas:

\begin{itemize}
    \item \textbf{DCMST:}
    \begin{itemize}
        \item $n: 120 \to 240$: tiempo crece $\sim$7.8x (teórico: 8x para $O(n^3)$)
        \item $n: 240 \to 480$: tiempo crece $\sim$2.6x
        \item $n: 480 \to 960$: tiempo crece $\sim$8.6x
    \end{itemize}

    \item \textbf{Lagrange:}
    \begin{itemize}
        \item $n: 120 \to 240$: tiempo crece $\sim$3.9x
        \item $n: 240 \to 480$: tiempo crece $\sim$6.5x
        \item Variabilidad debido al número de iteraciones del subgradiente
    \end{itemize}
\end{itemize}

El comportamiento de DCMST se ajusta razonablemente a su complejidad teórica $O(n^3)$.
El comportamiento de Lagrange es más errático debido a la naturaleza iterativa del
método del subgradiente, cuya convergencia depende de las características específicas
de cada instancia.

\subsection*{Análisis de Comportamiento de las Heurísticas}
\addcontentsline{toc}{subsection}{Análisis de Comportamiento de las Heurísticas}

Esta sección presenta un análisis teórico de las características estructurales de las
instancias que, según el diseño de los algoritmos, deberían favorecer o perjudicar
su desempeño. Este análisis se basa en la lógica interna de cada heurística.

\subsubsection*{Características de Instancias Favorables (Análisis Teórico)}

\paragraph{Algoritmo DCMST - Casos Teóricamente Favorables:}

Basándose en su estrategia greedy, el algoritmo DCMST debería desempeñarse mejor en:

\begin{enumerate}
    \item \textbf{Restricciones uniformes:} Cuando todos los vértices tienen restricciones
    similares ($d(v) \approx \bar{d}$ para todo $v$), el algoritmo greedy no enfrenta
    saturaciones prematuras y puede seleccionar libremente las aristas más baratas.

    \item \textbf{Costos bien distribuidos:} Si los costos siguen una distribución
    aproximadamente uniforme o normal, las aristas baratas no se concentran en
    vértices específicos, permitiendo al greedy construir soluciones balanceadas.

    \item \textbf{Grafos densos con alta conectividad:} En grafos completos o casi
    completos, existen múltiples alternativas de conexión, lo que reduce el impacto
    de decisiones greedy subóptimas.

    \item \textbf{Instancias pequeñas ($n < 20$):} Para grafos pequeños, el espacio
    de soluciones es reducido y el greedy explora implícitamente una fracción
    significativa de él.
\end{enumerate}

\paragraph{Algoritmo Lagrangiano - Casos Teóricamente Favorables:}

Por su diseño basado en relajación Lagrangiana, este algoritmo debería ser más efectivo cuando:

\begin{enumerate}
    \item \textbf{Restricciones moderadamente ajustadas:} Cuando $\sum d(v) \approx 2.5(n-1)$,
    el problema relajado captura bien la estructura del problema original y los
    multiplicadores convergen rápidamente.

    \item \textbf{Estructura de costos regular:} Cuando los costos no presentan
    outliers extremos (e.g., una arista con costo 1 y todas las demás con costo 100),
    el método del subgradiente converge establemente.

    \item \textbf{Grafos completos:} La garantía de factibilidad de KRUSKALX asegura
    que siempre se encuentra una solución, y la iteración del subgradiente la mejora
    progresivamente.

    \item \textbf{Simetría estructural:} En grafos con simetría (e.g., grids regulares,
    redes circulares), las soluciones óptimas tienden a ser balanceadas, favoreciendo
    la convergencia del dual Lagrangiano.
\end{enumerate}

\subsubsection*{Características de Instancias Desfavorables (Análisis Teórico)}

\paragraph{Algoritmo DCMST - Casos Teóricamente Desfavorables:}

\begin{enumerate}
    \item \textbf{Restricciones extremadamente heterogéneas:} Cuando conviven vértices
    con $d(v)=1$ junto a vértices con $d(v) \gg n/2$, el algoritmo debe balancear
    prioridades contradictorias y las decisiones greedy frecuentemente son subóptimas.

    \item \textbf{Costos adversarios:} Si las $k$ aristas más baratas inciden todas
    en un mismo vértice $v$ con $d(v) \ll k$, el greedy solo puede usar $d(v)$ de
    ellas, desperdiciando oportunidades de bajo costo.

    \item \textbf{Grafos dispersos:} En grafos con $m \approx 2n$ (apenas más aristas
    que el mínimo necesario), las opciones son limitadas y el greedy puede tomar
    decisiones irreversibles que impiden completar el árbol.

    \item \textbf{Dependencias de orden:} Cuando el orden óptimo de adición de aristas
    requiere añadir temporalmente aristas más caras para "reservar capacidad" en
    vértices críticos, el greedy por costo creciente falla.
\end{enumerate}

\paragraph{Algoritmo Lagrangiano - Casos Teóricamente Desfavorables:}

\begin{enumerate}
    \item \textbf{Restricciones muy holgadas:} Cuando $\sum d(v) \gg 2(n-1)$, el
    problema relajado se aproxima demasiado al MST clásico y los multiplicadores
    no proporcionan información útil para guiar hacia el DC-MST óptimo.

    \item \textbf{Convergencia lenta del subgradiente:} En instancias con estructura
    degenerada (e.g., múltiples soluciones óptimas con costos idénticos pero
    diferentes distribuciones de grado), el método puede oscilar sin converger.

    \item \textbf{Número limitado de iteraciones:} Si se impone un límite estricto
    de iteraciones para mantener eficiencia, el algoritmo puede detenerse antes
    de alcanzar el gap $< 1$ necesario para certificar optimalidad.
\end{enumerate}

\subsubsection*{Caso de Estudio: Fallo de Factibilidad en DCMST}

La siguiente instancia ilustra un caso extremo donde DCMST falla en encontrar cualquier
solución factible, a pesar de que existe:

\begin{lstlisting}
10 18
4 7 3
8 7 19
3 1 19
4 1 13
6 5 5
9 1 2
3 0 9
2 6 4
0 2 5
5 2 11
8 0 11
6 8 6
2 7 8
7 5 15
3 7 17
2 1 1
1 5 3
7 0 7
1 2 2 3 3 1 3 3 3 2
\end{lstlisting}

\paragraph{Análisis del Fallo:}

En esta instancia:
\begin{itemize}
    \item Varios vértices tienen restricciones muy ajustadas: $d \in \{1, 2\}$
    \item El algoritmo greedy selecciona aristas baratas (9-1: 2, 2-1: 1, 1-5: 3)
          que saturan prematuramente los vértices 1 y 2
    \item Al intentar conectar las componentes restantes, descubre que los únicos
          puentes disponibles requieren usar vértices ya saturados
    \item Las técnicas de intercambio (1-opt, 2-opt) no se pueden aplicar porque
          no se ha construido un árbol generador completo
\end{itemize}

\textbf{Solución con Lagrange:} En contraste, el algoritmo Lagrangiano, gracias a su
condición de no saturación en KRUSKALX, está diseñado teóricamente para prevenir
saturaciones prematuras de componentes en grafos completos, lo que sugiere que podría
manejar este tipo de instancias más robustamente.

\subsubsection*{Resumen Comparativo: DCMST vs Lagrange}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Característica} & \textbf{DCMST} & \textbf{Lagrange} \\
\hline
\hline
Complejidad teórica & $O(n^3)$ & $O(K \cdot n^3 \log n)$ \\
\hline
Escalabilidad observada & $n \approx 960$ & $n \approx 480$ \\
\hline
Garantía de factibilidad (teórica) & No & Sí (grafos completos) \\
\hline
Proporciona cotas de calidad & No & Sí ($z_{LB}$) \\
\hline
Velocidad (n=240) & 170.35 ms & 132.81 ms \\
\hline
Velocidad (n=480) & 435.05 ms & 862.33 ms \\
\hline
\end{tabular}
\caption{Comparación de características de las heurísticas (basado en datos experimentales de escalabilidad)}
\end{table}

\subsubsection*{Recomendaciones de Uso}

Basándose en el análisis experimental:

\begin{itemize}
    \item \textbf{Instancias pequeñas ($n \leq 15$):} Usar \textbf{Fuerza Bruta} para
    obtener la solución óptima garantizada.

    \item \textbf{Instancias medianas ($15 < n \leq 500$) con restricciones ajustadas:}
    Usar \textbf{Lagrange} para garantizar factibilidad y buena calidad.

    \item \textbf{Instancias grandes ($n > 500$):} Usar \textbf{DCMST} para maximizar
    escalabilidad. Si falla, recurrir a Lagrange o implementar backtracking en el greedy.

    \item \textbf{Aplicaciones de producción:} Combinar DCMST como primera aproximación
    rápida, seguido de Lagrange para refinamiento si se requiere mayor calidad.
\end{itemize}

\subsection*{Entorno de Pruebas y Compilación}
\addcontentsline{toc}{subsection}{Entorno de Pruebas y Compilación}

\subsubsection*{Requisitos del Sistema}

\begin{itemize}
    \item \textbf{Compilador:} g++ compatible con C++11 o superior
    \item \textbf{Sistema operativo:} Linux / WSL / macOS
    \item \textbf{Herramientas:} Make (opcional pero recomendado)
    \item \textbf{Dependencias:} Ninguna (implementación standalone)
\end{itemize}

\subsubsection*{Instrucciones de Compilación}

Para compilar y ejecutar los algoritmos implementados:

\begin{lstlisting}[language=bash]
# Clonar el repositorio
git clone https://github.com/kmy-cyber/DAA-Project.git
cd DAA-Project/src

# Compilar todos los algoritmos
make all

# Compilar y ejecutar todas las pruebas
make full

# Limpiar archivos compilados
make clean
\end{lstlisting}

\subsubsection*{Estructura del Proyecto}

\begin{verbatim}
DAA-Project/
├── src/              # Código fuente de los algoritmos
│   ├── brute.cpp     # Implementación de Fuerza Bruta
│   ├── dcmst.cpp     # Implementación de DCMST
│   ├── lagrange.cpp  # Implementación de Lagrange
│   └── Makefile      # Script de compilación
├── results/          # Resultados experimentales (CSV)
│   ├── brute_scalability.csv
│   ├── dcmst_scalability.csv
│   └── lagrange_scalability.csv
├── report/           # Documentación e informe
└── README.md         # Descripción del proyecto
\end{verbatim}

\subsubsection*{Generación de Instancias de Prueba}

Las instancias de prueba se generan aleatoriamente con las siguientes características:

\begin{itemize}
    \item Grafos completos $K_n$ con $n$ vértices
    \item Costos de aristas: enteros aleatorios uniformes en $[1, 100]$
    \item Restricciones de grado: generadas aleatoriamente garantizando:
    \begin{itemize}
        \item $d(v) \geq 1$ para todo $v \in V$
        \item $\sum_{v \in V} d(v) \geq 2(n-1)$ (condición de factibilidad)
    \end{itemize}
    \item Formato de entrada estándar:
    \begin{verbatim}
n m           # número de vértices y aristas
u1 v1 c1      # arista entre u1 y v1 con costo c1
u2 v2 c2
...
d[0] d[1] ... d[n-1]  # restricciones de grado
    \end{verbatim}
\end{itemize}

\newpage
\subsection*{Conclusiones}
\addcontentsline{toc}{subsection}{Conclusiones}

El problema de diseñar una red de fibra óptica que minimice costos respetando restricciones
de capacidad de puertos (\emph{Degree-Constrained Minimum Spanning Tree}) ha sido abordado
mediante un enfoque integral que combina análisis teórico, diseño algorítmico y
experimentación rigurosa.

\subsubsection*{Resultados Principales}

\begin{enumerate}
    \item \textbf{Complejidad Computacional:}
    Se demostró formalmente que el problema es NP-completo mediante una reducción desde
    \textsc{Hamiltonian Path}, confirmando que no existe un algoritmo polinomial exacto
    (salvo que P = NP). Esta demostración justifica rigurosamente el uso de heurísticas.

    \item \textbf{Algoritmo Exacto (Fuerza Bruta):}
    \begin{itemize}
        \item Complejidad: $O(2^m \cdot n)$
        \item Límite práctico: $n \approx 15$ vértices
        \item Útil para: Validación de heurísticas en instancias pequeñas
    \end{itemize}

    \item \textbf{Heurística DCMST:}
    \begin{itemize}
        \item Complejidad: $O(n^3)$
        \item Escalabilidad: Hasta $n = 960$ vértices en $<$ 4 segundos
        \item Limitaciones: No garantiza encontrar solución factible en todos los casos
        \item Aplicación: Instancias grandes donde la velocidad es crítica
    \end{itemize}

    \item \textbf{Heurística Lagrangiana:}
    \begin{itemize}
        \item Complejidad: $O(K \cdot n^3 \log n)$ con $K$ iteraciones
        \item Escalabilidad: Hasta $n = 480$ vértices en $<$ 1 segundo
        \item Ventajas: Garantiza factibilidad en grafos completos, proporciona cotas de calidad
        \item Aplicación: Instancias con restricciones ajustadas donde se requiere robustez
    \end{itemize}
\end{enumerate}

\subsubsection*{Lecciones Aprendidas}

\paragraph{Trade-offs Fundamentales:}
Los experimentos revelan un trade-off inherente entre escalabilidad y garantías de factibilidad.
DCMST es más escalable pero puede fallar en casos difíciles, mientras que Lagrange ofrece
mayor robustez a costa de mayor tiempo de cómputo.

\paragraph{Importancia del Análisis Experimental:}
El análisis teórico de complejidad, aunque esencial, no captura completamente el comportamiento
práctico. Las constantes ocultas en la notación $O(\cdot)$ y las características específicas
de las instancias tienen impacto significativo en el rendimiento real.

\paragraph{Diseño de Heurísticas Efectivas:}
Las heurísticas efectivas para problemas NP-duros requieren:
\begin{itemize}
    \item Fundamento teórico sólido (propiedades estructurales, relajaciones)
    \item Mecanismos de prevención de infactibilidad (look-ahead, saturación)
    \item Procedimientos de mejora local (intercambio de aristas)
    \item Validación experimental rigurosa
\end{itemize}

\subsubsection*{Aplicabilidad al Problema Original}

Para el caso real de diseño de la red de la Universidad de La Habana:

\begin{itemize}
    \item Con $\sim$50-100 edificios principales, ambas heurísticas son viables ($<$ 100ms).

    \item Se recomienda un enfoque híbrido:
    \begin{enumerate}
        \item Ejecutar DCMST para obtener solución rápida inicial
        \item Si DCMST falla o la calidad es insuficiente, ejecutar Lagrange
        \item Validar la solución final verificando restricciones y costo
    \end{enumerate}

    \item La garantía de factibilidad de Lagrange es valiosa para evitar rediseños
    costosos una vez iniciada la implementación física.
\end{itemize}

\subsubsection*{Trabajo Futuro}

Líneas de investigación prometedoras incluyen:

\begin{enumerate}
    \item \textbf{Algoritmos híbridos:} Combinar las fortalezas de DCMST (velocidad) y
    Lagrange (robustez) en un framework unificado.

    \item \textbf{Metaheurísticas avanzadas:} Explorar algoritmos genéticos, simulated
    annealing o búsqueda tabú específicamente diseñados para DC-MST.

    \item \textbf{Paralelización:} Explotar paralelismo en la enumeración de subconjuntos
    (Fuerza Bruta) o en las iteraciones del subgradiente (Lagrange).

    \item \textbf{Aproximaciones con garantías:} Desarrollar algoritmos con factor de
    aproximación garantizado para casos especiales del problema.

    \item \textbf{Aprendizaje automático:} Entrenar modelos que predigan qué heurística
    usar basándose en características de la instancia.
\end{enumerate}

\subsubsection*{Conclusión Final}

Este proyecto demuestra que problemas computacionalmente intratables (NP-duros) pueden
abordarse efectivamente mediante una combinación de rigor teórico, diseño algorítmico
creativo y experimentación sistemática. Las heurísticas desarrolladas ofrecen soluciones
prácticas y escalables para el diseño de redes de telecomunicaciones bajo restricciones
de capacidad, con aplicabilidad directa al problema de infraestructura de la Universidad
de La Habana y otros escenarios de diseño de redes del mundo real.

\subsection*{Referencias Bibliográficas}
\addcontentsline{toc}{subsection}{Referencias Bibliográficas}

\begin{enumerate}
    \item Cormen, T. H., Leiserson, C. E., Rivest, R. L., \& Stein, C. (2009).
    \textit{Introduction to Algorithms} (3rd ed.). MIT Press.

    \item Garey, M. R., \& Johnson, D. S. (1979).
    \textit{Computers and Intractability: A Guide to the Theory of NP-Completeness}.
    W. H. Freeman.

    \item Könemann, J., \& Ravi, R. (2003).
    \textit{Primal-dual meets local search: Approximating MSTs with nonuniform degree bounds}.
    SIAM Journal on Computing, 34(3), 763-783.

    \item Narula, S. C., \& Ho, C. A. (1980).
    \textit{Degree-constrained minimum spanning tree}.
    Computers \& Operations Research, 7(4), 239-249.

    \item Savelsbergh, M., \& Volgenant, T. (2005).
    \textit{Edge exchanges in the degree-constrained minimum spanning tree problem}.
    Computers \& Operations Research, 32(2), 341-355.

    \item Zhou, G., \& Gen, M. (1997).
    \textit{A note on genetic algorithms for degree-constrained spanning tree problems}.
    Networks, 30(2), 91-95.

    \item Caccetta, L., \& Hill, S. P. (2001).
    \textit{A branch and cut method for the degree-constrained minimum spanning tree problem}.
    Networks, 37(2), 74-83.

    \item Martínez, F. V., de Werra, D., \& Bentz, C. (2007).
    \textit{A Lagrangian heuristic for the degree-constrained minimum spanning tree problem}.
    European Journal of Operational Research, 176(1), 126-142.

    \item Kruskal, J. B. (1956).
    \textit{On the shortest spanning subtree of a graph and the traveling salesman problem}.
    Proceedings of the American Mathematical Society, 7(1), 48-50.

    \item Karger, D. R., Klein, P. N., \& Tarjan, R. E. (1995).
    \textit{A randomized linear-time algorithm to find minimum spanning trees}.
    Journal of the ACM, 42(2), 321-328.
\end{enumerate}

\vspace{1cm}

\noindent
\textit{Repositorio del proyecto:} \\
\url{https://github.com/kmy-cyber/DAA-Project}

\end{document}
