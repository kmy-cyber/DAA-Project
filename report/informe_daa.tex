\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}

\geometry{a4paper, left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\begin{document}

\begin{titlepage}
    \centering

    \vspace*{-1cm}
    \includegraphics[width=4cm]{matcom.jpeg}
    \vspace{1cm}

    {\Large \textbf{Universidad de La Habana}}\\[0.3cm]
    {\large \textbf{Facultad de Matemática y Computación}}\\[1cm]

    {\large Asignatura: Diseño y Análisis de Algoritmos}\\[2cm]

    {\LARGE \textbf{Conectando la UH}}\\[0.8cm]

    {\Large \textbf{Integrantes}}\\[0.6cm]
    {\large Jabel Resendiz Aguirre}\\
    {\large Noel Pérez Calvo}\\
    {\large Arianne Camila Palancar Ochando }\\[1.7cm]

    {\large Carrera: Ciencia de la Computación}\\[2cm]

    \vfill
    {\large \today}
\end{titlepage}


\tableofcontents
\newpage

\begin{center}
\LARGE \textbf{Problema}
\end{center}
\addcontentsline{toc}{section}{Problema}

La Universidad de La Habana, en su constante búsqueda de la excelencia académica y la innovación, 
se ha embarcado en un proyecto crucial para modernizar y expandir su infraestructura de red. Nuestro 
objetivo es dotar a todas nuestras facultades, centros de investigación y edificios administrativos con
conectividad de fibra óptica de alta velocidad. Para este fin, contamos con el valioso apoyo técnico y
logístico de ETECSA (Empresa de Telecomunicaciones de Cuba S.A.).

Nos enfrentamos a un desafío de diseño de red que requiere una solución óptima. Necesitamos 
interconectar todos los edificios principales de la UH con fibra óptica, creando una red robusta 
y eficiente. Cada posible conexión de fibra entre dos edificios tiene un costo de instalación asociado,
que incluye desde los permisos internos y la mano de obra especializada de ETECSA hasta los materiales
y las obras civiles necesarias.

Sin embargo, ETECSA ha establecido una restricción técnica fundamental que debemos respetar:  

En cada edificio, la conexión de la fibra óptica se gestionará a través de un equipo de red central 
(un router o switch principal) que ellos nos proporcionan. Estos equipos tienen una capacidad limitada 
de puertos. Esto significa que un equipo en un edificio específico solo puede manejar un número máximo 
de conexiones de fibra óptica directas a otros edificios. Exceder este límite implicaría la necesidad 
de instalar equipos adicionales mucho más caros y complejos, o la implementación de soluciones de red 
alternativas que ETECSA no puede garantizar o que dispararían drásticamente el presupuesto del proyecto.

Nuestro objetivo principal es diseñar la red de fibra óptica que conecte todos nuestros edificios 
principales de la manera más económica posible. Esto implica seleccionar las rutas de fibra de tal 
forma que:

\begin{enumerate}
	\item Todos los edificios estén interconectados a la red principal de la universidad, sin crear bucles 
innecesarios (buscamos una estructura de red eficiente).
	\item Ningún equipo de red en ningún edificio exceda su capacidad máxima de conexiones directas 
(es decir, el número de cables de fibra que llegan o salen de un edificio no puede superar el límite 
de puertos del equipo de ETECSA).
	\item El costo total de instalación de toda la red sea el mínimo posible.
\end{enumerate}

Una planificación subóptima podría resultar en un sobrecosto significativo para la universidad, la 
necesidad de adquirir hardware de red adicional no previsto, o en una red ineficiente que no cumpla 
con las especificaciones técnicas y presupuestarias acordadas con ETECSA.

\clearpage
\begin{center}
\LARGE \textbf{Formalización del Problema}
\end{center}
\addcontentsline{toc}{section}{Fase 1: Formalización del Problema}

El objetivo es diseñar una red de fibra óptica que conecte todos los edificios 
principales de la Universidad de La Habana mediante enlaces posibles provistos por ETECSA. 
Cada enlace tiene un costo de instalación y cada edificio posee un límite máximo 
de puertos disponibles. Se requiere encontrar una configuración de conexiones que:

\begin{itemize}
	\item conecte todos los edificios,
	\item respete los límites de puertos por edificio,
	\item minimice el costo total de instalación.
\end{itemize}

\subsection*{Notaci\'on}
\addcontentsline{toc}{subsection}{Notaci\'on}

En el lenguaje matemático, podemos definir la estructura del problema como un grafo simple y no dirigido $G=(V,E)$, donde:

\begin{itemize}
	\item Un conjunto de edificios:
	\[
	V = \{v_1, v_2, \ldots, v_n\}.
	\]
	
	\item Un conjunto de posibles enlaces de fibra óptica: $E=(e_1,e_2,...,e_m)$
	\[
	E \subseteq \{\{u,v\} \mid u,v \in V,\; u \neq v\}.
	\]
	
	\item Un costo de instalación para cada enlace:
	\[
	c : E \rightarrow \mathbb{R}_{>0}.
	\]
	
	\item Un límite de puertos (grado máximo permitido) en cada edificio:
	\[
	d : V \rightarrow \mathbb{Z}_{\ge 1}.
	\]
\end{itemize}

\subsection*{Restricciones del Problema}
\addcontentsline{toc}{subsection}{Restricciones del Problema}

Sea un subgrafo de G como \(T=(V^T,E^T)\) debe satisfacer que:

\begin{enumerate}

	\item \textbf{Conectividad:}
	\[
	T \text{ es conexo } \rightarrow V^T = V
	\]
	
	\item \textbf{Estructura de árbol:}
	\[
	T \text{ no contiene ciclos} \rightarrow |E^T| = |V| - 1.
	\]
	
	\item \textbf{Límite de puertos por edificio:}
	\[
	\deg_{T}(v) \le d(v), \qquad \forall v \in V.
	\]
\end{enumerate}

donde se define $deg_{T}(v)$ como el grado del v\'ertice $v$ (tambi\'en llamado cardinalidad de su vecindad) en el grafo T.
Se desea conseguir el subgrafo $T$ tal que minimiza el costo de la suma de los pesos de sus aristas, es decir:

\begin{enumerate}
	\item \textbf{Funci\'on objetivo:}
	\[
	\min_{T \subseteq E} \sum_{e \in T} c(e)
	\]
\end{enumerate}

sujeto a las restricciones anteriores.

\newpage
\begin{center}
\LARGE \textbf{Análisis de Complejidad Computacional}
\end{center}
\addcontentsline{toc}{section}{Fase 2: Análisis de Complejidad Computacional}

En esta fase se determina la dificultad computacional del problema formalizado en la Fase~1.
Demostraremos que la versión de decisión del problema es NP-completa, y por lo tanto, 
la versión de optimización es NP-dura. Este problema es conocido en la literatura como $DC-MST$, el cual
demostremos que la versi\'on de decisi\'on es NP-completa por lo que a versi\'on de optimizaci\'on lo es tambi\'en.

\subsection*{Versión de Decisión del Problema}

Dado un grafo $G=(V,E)$, costos $c(e)$, límites de grado $d(v)$ y un valor $K$, 
definimos la siguiente pregunta:

\[
\text{¿Existe un subconjunto } T \subseteq E \text{ tal que }
\begin{cases}
	G_T = (V,T) \text{ es un árbol}, \\
	\deg_{G_T}(v) \le d(v) \;\; \forall v \in V, \\
	\sum_{e \in T} c(e) \le K?
\end{cases}
\]

\subsection*{Pertenencia a NP}

Dado un conjunto de aristas $T$, quiere verificarse que se puede comprobar en tiempo polinomial, 
lo cual se cumplirá si se cumplen las siguientes condiciones:

\begin{itemize}
	\item $G_T$ es conexo (mediante un recorrido BFS/DFS),
	\item $|T| = |V|-1$, es decir el subgrafo $G_T$ forma un árbol,
	\item $\deg_{G_T}(v) \le d(v)$ para todo $v \in V$,
	\item $\sum_{e \in T} c(e) \le K$.
\end{itemize}

Por lo tanto, el problema pertenece a NP.

\subsection*{Demostración de NP-completitud}

Para demostrar que dicho problema es NP-completo, se presenta una 
reducción en tiempo polinomial desde el problema \textsc{Hamiltonian Path} en 
grafos no dirigidos, el cual es NP-completo.

\subsubsection*{Problema de Partida: Hamiltonian Path}

Dado un grafo no dirigido $G=(V,E)$, el problema \textsc{Hamiltonian Path} 
pregunta si existe un camino que visite todos los vértices exactamente una vez.

\subsubsection*{Construcción de la Reducción}

A partir de una instancia de \textsc{Hamiltonian Path}, construimos una instancia 
del problema de la siguiente manera:

\begin{itemize}
	\item Se toma el mismo grafo $G=(V,E)$.
	\item Para cada arista $e \in E$, se asigna un costo $c(e)=1$.
	\item Se fija un límite de grado uniforme:
	\[
	d(v) = 2 \quad \forall v \in V.
	\]
	\item Se establece el umbral de costo:
	\[
	K = |V| - 1.
	\]
\end{itemize}

La construcción es claramente polinomial.

\subsubsection*{Correctitud de la Reducción}

\paragraph{($\Rightarrow$)}  
Si $G$ posee un camino hamiltoniano, dicho camino contiene $|V|-1$ aristas, 
es conexo, acíclico y cada vértice tiene grado a lo sumo $2$.  
Por lo tanto, constituye un conjunto $T$ válido para el problema
con costo total $|V|-1 \le K$.

\paragraph{($\Leftarrow$)}  
Si existe un conjunto $T$ que satisface las restricciones del problema,
entonces $T$ es un árbol con $|V|-1$ aristas y $\deg_{G_T}(v) \le 2$ para todo $v$.  
La única estructura de árbol donde todos los grados son a lo sumo $2$ es un camino.  
Por lo tanto, $T$ es un camino hamiltoniano del grafo original.


Esto permite entonces reducir que:
\[
\textsc{DC-MST-Decision} \text{ es NP-completo}.
\]

Dado que la versión de decisión es NP-completa, la versión de optimización 
(\textit{Degree-Constrained Minimum Spanning Tree}) es NP-dura:

\[
\text{DC-MST-OPT es NP-dura}.
\]

Esto implica que no se conoce un algoritmo polinomial que resuelva el problema de forma
óptima para instancias generales, salvo que $\mathrm{P} = \mathrm{NP}$.

\newpage
\begin{center}
\LARGE \textbf{Diseño de Soluciones Algorítmicas}
\end{center}
\addcontentsline{toc}{section}{Fase 3: Diseño de Soluciones Algorítmicas}

El problema de construir un árbol generador con restricciones de grado
(DC-MST) es NP-difícil, lo que implica que no existe un algoritmo
polinomial conocido capaz de garantizar la solución óptima para
instancias generales. Por ello, en esta sección se exploran distintas
estrategias algorítmicas para abordar el problema, combinando
métodos exactos y heurísticos.

Presentaremos los algoritmos propuestos de manera estructurada,
incluyendo:

\begin{itemize}
    \item Una descripción de la idea principal de cada algoritmo.
    \item El pseudocódigo correspondiente para su implementación.
    \item El análisis de complejidad computacional.
\end{itemize}

Esto permitirá evaluar las ventajas y limitaciones de cada enfoque,
y proporcionará una base sólida para la implementación experimental
y la comparación de resultados en la Fase 4.

\subsection*{Algoritmo de Fuerza Bruta: Exploración Exhaustiva del Espacio de Árboles Generadores}
\addcontentsline{toc}{subsection}{Algoritmo de Fuerza Bruta: Exploración Exhaustiva del Espacio de Árboles gGeneradores}

Dado que el problema de encontrar un árbol generador de costo mínimo con
restricciones de grado es NP-difícil, una primera aproximación consiste en
analizar exhaustivamente todas las posibles soluciones y seleccionar aquella
que cumple las restricciones y minimiza el costo total.

Un candidato a solución válida debe ser un \textbf{árbol generador}, es decir,
un subgrafo conexo y acíclico que contiene todos los vértices del grafo y
exactamente $|V|-1$ aristas. El algoritmo de fuerza bruta consiste en enumerar
todos los subconjuntos de aristas del grafo y verificar cuáles de ellos cumplen
las condiciones necesarias para ser un árbol generador factible.

\subsubsection*{Idea del Algoritmo}

Sea $G=(V,E)$ un grafo no dirigido con $|V|=n$ vértices y $|E|=m$ aristas.
El procedimiento seguido es el siguiente:

\begin{enumerate}
    \item Enumerar todos los subconjuntos $S \subseteq E$.
    \item Descartar aquellos subconjuntos que no contengan exactamente $n-1$ aristas.
    \item Para cada subconjunto candidato:
    \begin{itemize}
        \item Verificar que el subgrafo inducido sea conexo y acíclico, utilizando
        una estructura de datos \emph{Disjoint Set Union} (DSU).
        \item Comprobar que el grado de cada vértice no exceda su límite máximo
        permitido.
        \item Calcular el costo total de las aristas seleccionadas.
    \end{itemize}
    \item Conservar el subconjunto válido cuyo costo total sea mínimo.
\end{enumerate}

Este algoritmo garantiza encontrar la solución óptima, aunque a costa de un
tiempo de ejecución exponencial.

\subsubsection*{Pseudocódigo}

\begin{lstlisting}[language=]
BruteForce-DCMST(G):
    bestCost <- +infinity
    bestTree <- empty

    for each subset S of E:
        if |S| != |V| - 1:
            continue

        initialize DSU with |V| elements
        deg[v] <- 0 for all v in V
        cost <- 0
        valid <- true

        for each edge (u,v) in S:
            if deg[u] + 1 > d(u) or deg[v] + 1 > d(v):
                valid <- false
                break

            deg[u] <- deg[u] + 1
            deg[v] <- deg[v] + 1

            if DSU.find(u) == DSU.find(v):
                valid <- false
                break

            DSU.union(u,v)
            cost <- cost + c(u,v)

        if valid and cost < bestCost:
            bestCost <- cost
            bestTree <- S

    return bestCost, bestTree
\end{lstlisting}

\subsubsection*{Análisis de Complejidad}

Sea $n = |V|$ el número de vértices y $m = |E|$ el número de aristas del grafo.

\begin{itemize}
    \item El algoritmo enumera todos los subconjuntos posibles de aristas, lo cual
    implica $2^m$ iteraciones.
    \item Para cada subconjunto candidato con $n-1$ aristas, se realizan:
    \begin{itemize}
        \item Operaciones de unión y búsqueda en la estructura DSU, con costo
        $O(n \alpha(n)) \approx O(n)$.
        \item Verificación de los límites de grado y cálculo del costo total,
        ambos en $O(n)$.
    \end{itemize}
\end{itemize}

Por tanto, la complejidad temporal total del algoritmo es:

\[
\boxed{
O(2^m \cdot n)
}
\]

En el peor caso, cuando el grafo es denso y $m = O(n^2)$, la complejidad crece
como $O(2^{n^2})$, lo cual hace que este enfoque sea impracticable para
instancias de tamaño moderado o grande.

\subsubsection*{Conclusión}

El algoritmo de fuerza bruta permite obtener la solución óptima del problema de
diseño de la red de fibra óptica bajo restricciones de grado. Sin embargo, su
costo computacional exponencial limita su uso a instancias muy pequeñas. Esto
justifica la necesidad de desarrollar algoritmos heurísticos o aproximados,
los cuales se abordan en las siguientes secciones.

\subsection*{Algoritmo Heurístico: Reducción del Árbol de Oportunidades}
\addcontentsline{toc}{subsection}{Algoritmo Heurístico: Reducción del Árbol de Oportunidades}

Con el objetivo de disminuir el tamaño del espacio de búsqueda y la dificultad
computacional del problema, se introducen una serie de propiedades estructurales
del \textbf{árbol generador de costo mínimo con restricciones de grado} que permiten
reducir el grafo original sin perder optimalidad.

Sea $T^*$ un árbol generador de costo mínimo con restricciones de grado del grafo
$G = (V,E)$.

\subsubsection*{Propiedades Estructurales}

\paragraph{Lema 1.}
Sea $v \in V$ un vértice hoja, es decir, un vértice con grado uno en el grafo
original $G$. Entonces, la única arista incidente a $v$ debe pertenecer a $T^*$.

\paragraph{Demostración.}
Dado que $T^*$ es un grafo conexo que contiene todos los vértices de $G$, el vértice
$v$ debe estar conectado al resto del árbol mediante su única arista incidente.
Excluir dicha arista implicaría que $v$ quedaría aislado, contradiciendo la
conectividad de $T^*$. Por tanto, toda arista incidente a un vértice colgante debe
incluirse necesariamente en $T^*$. \hfill $\square$

\vspace{0.2cm}

\paragraph{Lema 2.}
Sea $V_1 = \{ v_i \in V \mid d(v_i) = 1 \}$ el conjunto de vértices cuyo grado máximo
permitido es uno, y sea
\[
E_1 = \{ (v_i,v_j) \in E \mid v_i, v_j \in V_1 \}.
\]
Si $|V| > 2$, entonces ninguna arista de $E_1$ puede pertenecer a $T^*$.

\paragraph{Demostración.}
Supóngase que existe una arista $(v_i,v_j) \in E_1$ incluida en $T^*$. Como
$d(v_i)=d(v_j)=1$, ninguno de estos vértices puede conectarse con ningún otro
vértice adicional en $T^*$. Esto implica que el subgrafo resultante no puede ser
conexo cuando $|V|>2$, lo cual contradice la definición de árbol generador.
Por tanto, todas las aristas de $E_1$ deben ser excluidas de $T^*$. \hfill $\square$

\vspace{0.2cm}

\paragraph{Lema 3.}
Sea $v_k$ un vértice de grado dos en $G$, con vecinos $v_i$ y $v_j$. Si no existe
ningún camino entre $v_i$ y $v_j$ que no pase por $v_k$, entonces las aristas
$(v_k,v_i)$ y $(v_k,v_j)$ deben pertenecer a $T^*$.

\paragraph{Demostración.}
Supóngase que alguna de las aristas $(v_k,v_i)$ o $(v_k,v_j)$ no pertenece a $T^*$.
Dado que no existe un camino alternativo entre $v_i$ y $v_j$ que evite el vértice
$v_k$, se deduce que $T^*$ no puede ser conexo, lo cual contradice su definición.
Por tanto, ambas aristas deben incluirse necesariamente en $T^*$. \hfill $\square$

---

\subsubsection*{Algoritmo de Reducción}

A partir de las propiedades anteriores, se define un procedimiento de reducción
que simplifica el grafo original antes de aplicar un algoritmo constructivo.

\begin{lstlisting}
Reduction_DCMST(G = (V,E), d):
    T* <- empty
    1. Eliminar todas las aristas que satisfacen el Lema 2.
    2. Identificar todos los v\'ertices colgantes (grado 1),
       agregar sus aristas incidentes a T* y eliminarlos del grafo.
    3. Identificar v\'ertices que satisfacen el Lema 3,
       agregar las aristas correspondientes a T* y eliminarlas del grafo.
    return G reducido, T*
\end{lstlisting}

El costo computacional de este algoritmo de reducción es:

\begin{itemize}
    \item Paso 1: $O(n^2)$,
    \item Paso 2: $O(n)$,
    \item Paso 3: $O(n^3)$.
\end{itemize}

Por tanto, la complejidad temporal total del algoritmo de reducción es:

\[
O(n^3)
\]

---

\subsubsection*{Construcción del Árbol con Restricciones de Grado}

Una vez reducido el grafo, se construye un árbol generador utilizando un algoritmo
greedy basado en el algoritmo clásico de Kruskal, adaptado para respetar las
restricciones de grado.

El algoritmo itera seleccionando aristas de menor costo que conecten componentes
distintas y que no violen los límites de grado de los vértices involucrados. Este
procedimiento continúa hasta obtener $|V|-1$ aristas.


\begin{lstlisting}
MAIN_DCMST(G = (V, E), w, d):
    V* <- V
    E* <- empty
    T* <- (V*, E*)

    1. Ejecutar REDUCTION_DCMST(G, d)
    
    2. E1 <- E

    3. while |E*| < |V| - 1 do
           seleccionar la arista de menor costo
           emin = (vk, vh) en E1
           
           eliminar emin de E1
           
           if vk y vh estan en componentes distintas de T*
              and deg_T*(vk) < d(vk)
              and deg_T*(vh) < d(vh) then
               
               E* <- E* U {emin}
               unir las componentes de vk y vh en T*
           end if
       end while

    4. Aplicar tecnicas de intercambio de aristas
       (1-opt y 2-opt) para mejorar la solucion

    return G* = (V*, E*)
\end{lstlisting}


---

\subsubsection*{Técnicas de Intercambio de Aristas}

Para mejorar la solución obtenida, se emplean técnicas de \emph{edge exchange}.

\paragraph{Intercambio 1-opt.}
Consiste en eliminar una arista del árbol y reemplazarla por una arista externa,
siempre que el resultado sea un árbol. Esta operación puede modificar los grados
de los vértices y, por tanto, debe verificarse nuevamente la restricción de grado.

\paragraph{Intercambio 2-opt.}
Consiste en reemplazar dos aristas del árbol por dos aristas externas, de forma
que el grado de cada vértice se conserve. Esta operación garantiza que la
restricción de grado siga siendo satisfecha y permite mejorar el costo total del
árbol.

---

\subsubsection*{Complejidad del Algoritmo Heurístico}

La complejidad temporal del algoritmo completo es la siguiente:

\begin{itemize}
    \item Reducción del grafo: $O(n^3)$.
    \item Construcción inicial del árbol (Kruskal modificado): $O(n)$.
    \item Intercambio 1-opt: $O(n)$.
    \item Intercambio 2-opt: $O(n^2)$.
\end{itemize}

Por tanto, la complejidad temporal total del algoritmo heurístico es:

\[
\boxed{O(n^3)}
\]

Este enfoque permite obtener soluciones de alta calidad en tiempo polinomial,
haciendo viable el tratamiento de instancias de tamaño considerable.

\subsection*{Heurística Lagrangiana para DC-MST}
\addcontentsline{toc}{subsection}{Heurística Lagrangiana para DC-MST}

El enfoque de relajación Lagrangiana constituye una técnica poderosa para 
obtener cotas inferiores de calidad y guiar la construcción de soluciones 
factibles para problemas de optimización combinatoria NP-duros. En el contexto
del DC-MST, se aplica esta técnica relajando las restricciones de grado 
mediante multiplicadores Lagrangianos, lo que transforma el problema original
en un problema de árbol generador de costo mínimo (MST) clásico, resoluble en tiempo
polinomial.

\subsubsection*{Idea del Algoritmo}

La heurística Lagrangiana propuesta se compone de tres componentes principales
que trabajan de forma integrada:

\paragraph{1. Relajación Lagrangiana del DC-MST.}
Se relajan las restricciones de grado mediante multiplicadores Lagrangianos
$\lambda = (\lambda_1, \lambda_2, \ldots, \lambda_n) \in \mathbb{R}_+^{|V|}$,
donde $\lambda_i$ penaliza las violaciones de la restricción de grado en el
vértice $i \in V$.

El subproblema de relajación Lagrangiana (LRS) resultante es:

\[
z(\lambda) = \min \left\{ \sum_{e=[i,j] \in E} (c_e + \lambda_i + \lambda_j) x_e 
- \sum_{i \in V} \lambda_i d_i : x \in R_0 \right\}
\]

donde $R_0$ representa el conjunto de vectores de incidencia de árboles 
generadores de $G$. Este subproblema equivale a encontrar un MST con costos
modificados $\{c_e + \lambda_i + \lambda_j : e=[i,j] \in E\}$, resoluble 
eficientemente mediante algoritmos de Kruskal o Prim.

Para obtener la mejor cota inferior posible, se resuelve el problema dual
Lagrangiano:

\[
z(\lambda^*) = \max_{\lambda \geq 0} \left\{ z(\lambda) \right\}
\]

El vector óptimo $\lambda^*$ se obtiene mediante el \textbf{método del 
subgradiente}, que actualiza iterativamente los multiplicadores según:

\[
\lambda_i^{k+1} = \max\left\{0, \lambda_i^k + t^k s_i^k\right\}, 
\quad \forall i \in V
\]

donde $s_i^k = \sum_{e \in \delta(i)} x_e^k - d_i$ es el subgradiente
(violación de la restricción de grado en el vértice $i$), y el tamaño de
paso $t^k$ se calcula como:

\[
t^k = \frac{(1 + \alpha) z_{UB} - z(\lambda^k)}{\|s^k\|^2}
\]

siendo $z_{UB}$ una cota superior conocida y $\alpha$ un parámetro de 
control (típicamente $\alpha = 0.03$).

\paragraph{2. Construcción Heurística con Prevención de Infactibilidad.}
Se introduce el algoritmo \texttt{KRUSKALX}, una variante del algoritmo
clásico de Kruskal que incorpora un mecanismo de \emph{look-ahead} para
prevenir la generación de soluciones infactibles.

La idea fundamental es el concepto de \textbf{saturación}. Sea $T=(V_T, E_T)$
un subárbol. Se definen:

\begin{itemize}
    \item \textbf{Grado del árbol:} $\delta(T) = \sum_{i \in V_T} \sum_{e \in \delta(i) \cap E_T} 1$
    \item \textbf{Capacidad del árbol:} $d(T) = \sum_{i \in V_T} d_i$
\end{itemize}

Un árbol $T$ se considera \textbf{saturado} si $\delta(T) = d(T)$, es decir,
si ha alcanzado su capacidad máxima de aristas respetando las restricciones
de grado.

\textbf{Proposición (Condición de No Saturación):}
Sean $T_1$ y $T_2$ dos subárboles disjuntos no saturados. Si existe una arista
$e \in E$ tal que $T_3 = (V_1 \cup V_2, E_1 \cup E_2 \cup \{e\})$ es también
no saturado, entonces:

\[
\delta(T_3) = \delta(T_1) + \delta(T_2) + 2 < d(T_3)
\]

Esta propiedad garantiza que en grafos completos siempre es posible construir
una solución factible, incluso cuando algunos vértices tienen $d_i \in \{1,2\}$.

El algoritmo \texttt{KRUSKALX} ordena las aristas por costo creciente y las
considera secuencialmente. Una arista $e=[i,j]$ se añade al árbol en
construcción si y solo si:

\begin{enumerate}
    \item Conecta dos componentes distintas (no crea ciclos)
    \item No viola las restricciones de grado: $\deg(i) < d_i$ y $\deg(j) < d_j$
    \item La unión resultante no está saturada (excepto en la última arista)
\end{enumerate}

Para guiar la construcción hacia soluciones de buena calidad, se utilizan
\textbf{costos complementarios}. Si $x^k$ es la solución del LRS en la
iteración $k$, los costos se modifican como:

\[
\bar{c}_e = (1 - x_e^k) c_e, \quad \forall e \in E
\]

Esto hace que las aristas seleccionadas en la solución Lagrangiana sean
más atractivas para el algoritmo constructivo.

\paragraph{3. Problema Restringido.}
Para mejorar la eficiencia computacional, se define un \textbf{problema 
restringido} sobre un subconjunto de aristas $E' \subseteq E$. Dado el
ordenamiento de aristas $\{e_1, e_2, \ldots, e_m\}$ usado por 
\texttt{KRUSKALX} y siendo $k^*$ el índice de la última arista insertada,
se define:

\[
E' = \{e_1, e_2, \ldots, e_{m^*}\}, \quad \text{donde } m^* = \min\{2k^*, m\}
\]

El grafo restringido $G' = (V, E')$ contiene típicamente una fracción pequeña
de las aristas originales pero preserva soluciones de alta calidad. Trabajar
sobre $G'$ reduce significativamente el tiempo de cómputo permitiendo abordar
instancias con miles de vértices.

\paragraph{4. Procedimiento de Mejora Local.}
Una vez construida una solución factible $T$, se aplica un procedimiento de
mejora por intercambio de aristas. Para cada arista $e \in T$:

\begin{enumerate}
    \item Se elimina $e$ de $T$, generando dos subárboles $T_1$ y $T_2$
    \item Se busca la arista de menor costo $\bar{e} \in E \setminus T$ que 
          reconecte $T_1$ y $T_2$ sin violar restricciones de grado
    \item Si $c_{\bar{e}} < c_e$, se realiza el intercambio: 
          $T \leftarrow (T \setminus \{e\}) \cup \{\bar{e}\}$
\end{enumerate}

Este procedimiento tiene complejidad $O(|V| \cdot |E|)$ y se ejecuta sobre
el árbol obtenido en cada iteración del método del subgradiente.

\subsubsection*{Pseudocódigo}

\begin{lstlisting}
KRUSKALX(G, c, d):
    Ordenar aristas E = {e1, e2, ..., em} por costo creciente
    Inicializar deg[i] <- 0 para todo i en V
    Inicializar T <- (V, empty), k <- 1
    
    while |E(T)| < |V| - 1 and k <= m:
        ek <- [i, j]
        
        if deg[i] < d[i] and deg[j] < d[j]:
            if ek no forma ciclo en T:
                if |E(T)| = |V| - 2:
                    /* Ultima arista, agregar directamente */
                    E(T) <- E(T) U {ek}
                    deg[i] <- deg[i] + 1
                    deg[j] <- deg[j] + 1
                else:
                    /* Verificar condicion de no saturacion */
                    T1 <- componente de i en T
                    T2 <- componente de j en T
                    if delta(T1) + delta(T2) + 2 < d(T1) + d(T2):
                        E(T) <- E(T) U {ek}
                        deg[i] <- deg[i] + 1
                        deg[j] <- deg[j] + 1
                    end if
                end if
            end if
        end if
        k <- k + 1
    end while
    
    return T, k*
\end{lstlisting}

\vspace{0.3cm}

\begin{lstlisting}
IMPROVEMENT_PROCEDURE(T, G, c, d):
    E0 <- E(T)
    improved <- true
    
    while improved:
        improved <- false
        for each e = [i,j] in E0:
            Eliminar e de T, obteniendo T1 y T2
            
            /* Buscar mejor arista de reconexion */
            e_best <- null
            cost_best <- +infinito
            
            for each e' = [u,v] in E \ E(T):
                if u en V(T1) and v en V(T2) (o viceversa):
                    if deg_T(u) < d[u] or u in {i,j}:
                        if deg_T(v) < d[v] or v in {i,j}:
                            if c[e'] < cost_best:
                                e_best <- e'
                                cost_best <- c[e']
                            end if
                        end if
                    end if
                end if
            end for
            
            if e_best != null and cost_best < c[e]:
                E(T) <- (E(T) \ {e}) U {e_best}
                improved <- true
            else:
                Restaurar e en T
            end if
        end for
    end while
    
    return T
\end{lstlisting}

\vspace{0.3cm}

\begin{lstlisting}
LAGRANGIAN_HEURISTIC(G, c, d):
    /* Fase 1: Inicializacion */
    T_init <- KRUSKALX(G, c, d)
    E' <- primeras 2*k* aristas del ordenamiento
    G' <- (V, E')
    
    lambda[i] <- 0 para todo i en V
    z_LB <- -infinito
    z_UB <- costo(T_init)
    T_best <- T_init
    
    alpha <- 2.0
    iter <- 0
    max_iter <- 1000
    iter_sin_mejora <- 0
    
    /* Fase 2: Metodo del subgradiente */
    while iter < max_iter and z_UB - z_LB > 0.99:
        /* Resolver LRS con costos modificados */
        c_mod[e=[i,j]] <- c[e] + lambda[i] + lambda[j]
        T_LRS <- MST(G', c_mod)
        z_lambda <- costo(T_LRS) - sum(lambda[i] * d[i])
        
        if z_lambda > z_LB:
            z_LB <- z_lambda
            iter_sin_mejora <- 0
        else:
            iter_sin_mejora <- iter_sin_mejora + 1
        end if
        
        /* Si LRS es factible para DC-MST */
        if deg_T_LRS[i] <= d[i] para todo i en V:
            T_LRS <- IMPROVEMENT_PROCEDURE(T_LRS, G', c, d)
            if costo(T_LRS) < z_UB:
                z_UB <- costo(T_LRS)
                T_best <- T_LRS
            end if
        end if
        
        /* Construccion heuristica con costos complementarios */
        c_comp[e] <- (1 - x_LRS[e]) * c[e]
        T_heur <- KRUSKALX(G', c_comp, d)
        
        if T_heur es factible:
            T_heur <- IMPROVEMENT_PROCEDURE(T_heur, G', c, d)
            if costo(T_heur) < z_UB:
                z_UB <- costo(T_heur)
                T_best <- T_heur
            end if
        end if
        
        /* Actualizar multiplicadores Lagrangianos */
        s[i] <- deg_T_LRS[i] - d[i] para todo i en V
        t <- alpha * (z_UB - z_lambda) / ||s||^2
        
        for i = 1 to |V|:
            if s[i] > 0 or lambda[i] > 0:
                lambda[i] <- max(0, lambda[i] + t * s[i])
            end if
        end for
        
        /* Ajustar parametro alpha */
        if iter_sin_mejora > 30:
            alpha <- alpha * 0.5
            iter_sin_mejora <- 0
        end if
        
        iter <- iter + 1
    end while
    
    return T_best, z_LB, z_UB
\end{lstlisting}

\subsubsection*{Análisis de Complejidad}

Sea $n = |V|$, $m = |E|$ y $m' = |E'|$ el tamaño del problema restringido.

\paragraph{Complejidad de KRUSKALX:}
\begin{itemize}
    \item Ordenamiento de aristas: $O(m \log m)$
    \item Construcción del árbol con verificación de saturación: $O(m \alpha(n))$
    \item Complejidad total: $O(m \log m + m \alpha(n)) = O(m \log m)$
\end{itemize}

\paragraph{Complejidad del Procedimiento de Mejora:}
\begin{itemize}
    \item Para cada arista en $T$: $O(n)$ iteraciones
    \item Búsqueda de mejor reconexión: $O(m')$
    \item Complejidad por iteración: $O(n \cdot m')$
    \item En el peor caso (múltiples pasadas): $O(n^2 \cdot m')$
\end{itemize}

\paragraph{Complejidad de una Iteración del Método del Subgradiente:}
\begin{itemize}
    \item Resolver LRS (MST): $O(m' \log m')$
    \item Ejecutar KRUSKALX: $O(m' \log m')$
    \item Procedimiento de mejora: $O(n^2 \cdot m')$
    \item Actualización de multiplicadores: $O(n)$
    \item Total por iteración: $O(n^2 \cdot m' + m' \log m')$
\end{itemize}

\paragraph{Complejidad Total del Algoritmo:}

Sea $K$ el número de iteraciones del método del subgradiente (típicamente 
$K = O(1000)$ en la práctica). La complejidad total es:

\[
\boxed{O\left(K \cdot (n^2 \cdot m' + m' \log m')\right)}
\]

Para grafos completos, $m = \Theta(n^2)$ y típicamente $m' = O(n \log n)$
debido al problema restringido. En este caso:

\[
O(K \cdot n^3 \log n)
\]

Esta complejidad es manejable para instancias con miles de vértices, como
demuestran los experimentos computacionales reportados en la literatura,
donde se resuelven instancias con hasta 2000 vértices.

\paragraph{Ventajas del Enfoque Lagrangiano:}

\begin{enumerate}
    \item \textbf{Cotas de calidad:} Proporciona cotas inferiores que permiten
    evaluar la calidad de las soluciones heurísticas.
    
    \item \textbf{Guiado inteligente:} La información dual guía la construcción
    de soluciones factibles hacia regiones prometedoras del espacio de búsqueda.
    
    \item \textbf{Prueba de optimalidad:} Cuando $z_{UB} - z_{LB} < 1$ (para
    costos enteros), se certifica que la solución es óptima.
    
    \item \textbf{Escalabilidad:} El problema restringido permite trabajar con
    grafos grandes manteniendo la calidad de las soluciones.
    
    \item \textbf{Robustez:} Garantiza encontrar soluciones factibles en grafos
    completos, incluso con restricciones de grado muy ajustadas ($d_i \in \{1,2\}$).
\end{enumerate}

\newpage
\begin{center}
\LARGE \textbf{Implementación y Análisis Experimental}
\end{center}
\addcontentsline{toc}{section}{Fase 4: Implementación y Análisis Experimental}

En esta fase se presenta la implementación práctica de los tres algoritmos propuestos y se realiza
un análisis experimental exhaustivo para evaluar su comportamiento en términos de calidad de solución,
tiempo de ejecución y escalabilidad.

\subsection*{Metodología de Experimentación}
\addcontentsline{toc}{subsection}{Metodología de Experimentación}

Los experimentos se realizaron bajo las siguientes condiciones:

\begin{itemize}
    \item \textbf{Lenguaje:} C++ (estándar C++11)
    \item \textbf{Compilador:} g++ con optimizaciones -O2
    \item \textbf{Estructura de datos:} 
    \begin{itemize}
        \item Disjoint Set Union (DSU) con path compression y union by rank
        \item Listas de adyacencia para representación de grafos
    \end{itemize}     
    \item \textbf{Límites por instancia:} 
    \begin{itemize}
        \item Tiempo de ejecución: 5 segundos (5000 ms)
        \item Memoria: 512 MB
    \end{itemize}
    \item \textbf{Métricas evaluadas:} Tiempo de ejecución, calidad de solución, escalabilidad y consumo de memoria.
    \item \textbf{Estados posibles de una instancia:} 
    \begin{itemize}
        \item \textbf{OK:}solución óptima o mejor conocida
        \item \textbf{BAD:} solución subóptima
        \item \textbf{WA:} fallo en el checker (resultado incorrecto)
        \item \textbf{TLE:} tiempo excedido
        \item \textbf{MLE:} memoria excedida
    \end{itemize}
\end{itemize}

\subsection*{Diseño y generación de instancias de prueba}
\addcontentsline{toc}{subsection}{Diseño y generación de instancias de prueba}

Para evaluar de manera rigurosa los algoritmos y heurísticas desarrolladas para el problema 
\emph{Degree-Constrained Minimum Spanning Tree (DCMST)}, se diseñó un generador de instancias 
de prueba que permite crear conjuntos de grafos controlados y reproducibles. 

El generador de instancias sigue las siguientes características principales:

\begin{itemize}
    \item \textbf{Grafos aleatorios controlados}: el número de nodos y aristas se determina de manera aleatoria dentro de rangos predefinidos, asegurando que cada instancia sea un grafo conexo y válido para el problema.
    \item \textbf{Asignación de pesos aleatorios mediante Mersenne Twister}: los pesos de las aristas se generan de forma pseudoaleatoria utilizando el generador Mersenne Twister (\texttt{std::mt19937}), lo que permite reproducibilidad y control sobre el rango de valores.
    \item \textbf{Distribución de grados de nodos}: para cada nodo, se asigna un grado aleatorio dentro de un rango determinado, simulando distintos niveles de conectividad y heterogeneidad en el grafo.
\end{itemize}

Este enfoque ofrece varias ventajas:

\begin{enumerate}
    \item \textbf{Reproducibilidad}: al fijar la semilla del generador de números aleatorios, las instancias pueden regenerarse exactamente, facilitando comparaciones entre algoritmos.
    \item \textbf{Diversidad de casos de prueba}: el generador permite variar el número de nodos, la cantidad de aristas y el rango de pesos, cubriendo un amplio espectro de escenarios posibles.
    \item \textbf{Control sobre propiedades críticas}: se pueden definir casos específicos que maximicen la dificultad del problema, como grafos densos con pesos heterogéneos, evaluando la eficacia de las heurísticas en situaciones extremas.
    \item \textbf{Simplicidad y eficiencia}: al usar un solo modelo de grafo aleatorio con Mersenne Twister, se reduce la complejidad de implementación y se garantiza rapidez en la generación de instancias grandes.
\end{enumerate}

De este modo, el conjunto de instancias generado combina \emph{casos aleatorios representativos} con 
\emph{casos de estrés diseñados}, asegurando una evaluación robusta y completa de los algoritmos de DCMST.

\subsection*{Descripción del proceso del juez automático}
\addcontentsline{toc}{subsection}{Descripción del proceso del juez automático}
El sistema realiza los siguientes pasos:

\begin{enumerate}
    \item Recolecta todas las instancias de prueba en la carpeta \texttt{tests/}.
    \item Ejecuta cada algoritmo sobre cada instancia, midiendo:
    \begin{itemize}
        \item Tiempo real de ejecución con \texttt{/usr/bin/time}
        \item Memoria máxima usada
        \item Costo de la solución generada
    \end{itemize}
    \item Verifica la corrección de la solución usando un \emph{checker} externo.
    \item Clasifica el resultado de cada ejecución en uno de los estados mencionados.
    \item Guarda los resultados intermedios y finales en el repositorio de la siguiente manera:
    \begin{itemize}
        \item Las soluciones generadas por cada algoritmo se almacenan en la carpeta \texttt{outputs/}, 
              con un archivo por algoritmo e instancia: \texttt{outputs/<algoritmo>\_<instancia>.out}.
        \item Los registros de ejecución, incluyendo tiempo, memoria y estado de la solución, se guardan 
        en \texttt{logs/}, un archivo CSV por algoritmo: \texttt{logs/<algoritmo>.csv}.
        \item Finalmente, se genera un resumen global en formato Markdown llamado \texttt{output.md}, que 
        resume para cada algoritmo el número de ejecuciones óptimas, subóptimas, errores de checker, 
        tiempos excedidos y límites de memoria alcanzados.
    \end{itemize}
\end{enumerate}

Este sistema permite automatizar la evaluación de múltiples algoritmos y variantes heurísticas, controlando tanto el tiempo como la memoria utilizada, y asegurando que las soluciones sean correctas antes de ser consideradas óptimas o subóptimas

\subsection*{Tamaño Máximo de Instancia para Fuerza Bruta}
\addcontentsline{toc}{subsection}{Tamaño Máximo de Instancia para Fuerza Bruta}

El algoritmo de fuerza bruta, debido a su complejidad exponencial $O(2^m \cdot n)$, presenta
limitaciones severas en términos de escalabilidad. Los experimentos revelan que:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Tamaño (n)} & \textbf{Tiempo (ms)} & \textbf{Estado} \\
\hline
5 & 5.084 & OK \\
10 & 8.090 & OK \\
15 & 6.562 & OK \\
20 & 61890.611 & TLE \\
\hline
\end{tabular}
\caption{Escalabilidad del algoritmo de Fuerza Bruta}
\end{table}

\paragraph{Conclusión:} El límite práctico de viabilidad para el algoritmo de fuerza bruta
se encuentra en $\mathbf{n \approx 15}$ vértices. Para $n=20$, el tiempo de ejecución excede
ampliamente los 5 segundos de timeout establecido, alcanzando 61.89 segundos. Este comportamiento
confirma la naturaleza exponencial del algoritmo y justifica plenamente la necesidad de
algoritmos heurísticos para instancias de tamaño real.

El crecimiento explosivo del tiempo de ejecución entre $n=15$ y $n=20$ ($\sim$9400x) evidencia
que cada incremento en el tamaño del grafo multiplica el espacio de búsqueda de forma
exponencial. Para grafos completos, pasar de 15 a 20 nodos significa evaluar $2^{190}$ vs
$2^{105}$ subconjuntos potenciales de aristas.

\subsection*{Análisis Empírico Comparativo}
\addcontentsline{toc}{subsection}{Análisis Empírico Comparativo}

A continuación se presenta un análisis comparativo de los tres enfoques implementados en
términos de tiempo de ejecución y escalabilidad.

\subsubsection*{Comparación de Tiempos de Ejecución}

Para instancias pequeñas donde todos los algoritmos son viables ($n \leq 15$), se observa:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{n} & \textbf{Brute Force (ms)} & \textbf{DCMST (ms)} & \textbf{Lagrange (ms)} \\
\hline
5 & 5.084 & 3.582 & 2.800 \\
10 & 8.090 & 3.515 & 3.545 \\
15 & 6.562 & 4.421 & 5.658 \\
\hline
\end{tabular}
\caption{Comparación de tiempos en instancias pequeñas}
\end{table}

\paragraph{Observaciones:}
\begin{itemize}
    \item Para $n \leq 15$, todos los algoritmos presentan tiempos comparables (milisegundos).
    \item Las heurísticas (DCMST y Lagrange) son consistentemente más rápidas que Fuerza Bruta.
    \item Lagrange muestra ventaja en instancias muy pequeñas, pero DCMST es más consistente.
\end{itemize}


\subsubsection*{Escalabilidad de las Heurísticas}

Para instancias de tamaño medio y grande, comparamos DCMST y Lagrange:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{n} & \textbf{DCMST (ms)} & \textbf{Estado} & \textbf{Lagrange (ms)} & \textbf{Estado} \\
\hline
20 & 5.497 & OK & 3.771 & OK \\
30 & 4.819 & OK & 5.239 & OK \\
60 & 6.264 & OK & 8.490 & OK \\
120 & 21.752 & OK & 34.431 & OK \\
240 & 170.350 & OK & 132.808 & OK \\
480 & 435.045 & OK & 862.332 & OK \\
960 & 3758.735 & OK & 6752.041 & TLE \\
1920 & 32250.960 & TLE & --- & --- \\
\hline
\end{tabular}
\caption{Escalabilidad comparativa de las heurísticas}
\end{table}

\paragraph{Análisis de Resultados:}

\begin{enumerate}
    \item \textbf{DCMST (Reducción del Árbol de Oportunidades):}
    \begin{itemize}
        \item Escala exitosamente hasta $n=960$ con tiempo $\sim$3.76 segundos.
        \item Para $n=1920$, excede el timeout (32.25 segundos).
        \item Complejidad $O(n^3)$ observable: duplicar $n$ multiplica el tiempo $\sim$8x.
        \item Es el algoritmo más escalable de los tres implementados.
    \end{itemize}

    \item \textbf{Lagrange (Relajación Lagrangiana):}
    \begin{itemize}
        \item Funciona eficientemente hasta $n=480$ ($\sim$862ms).
        \item Para $n=960$, excede el timeout (6.75 segundos).
        \item A pesar de su sofisticación teórica, el overhead de las iteraciones del
              subgradiente y las operaciones de mejora limitan su escalabilidad.
        \item El problema restringido ($E'$) crece significativamente en grafos grandes.
    \end{itemize}
\end{enumerate}

\subsubsection*{Análisis de Crecimiento Temporal}

Calculando las tasas de crecimiento observadas:

\begin{itemize}
    \item \textbf{DCMST:}
    \begin{itemize}
        \item $n: 120 \to 240$: tiempo crece $\sim$7.8x (teórico: 8x para $O(n^3)$)
        \item $n: 240 \to 480$: tiempo crece $\sim$2.6x
        \item $n: 480 \to 960$: tiempo crece $\sim$8.6x
    \end{itemize}

    \item \textbf{Lagrange:}
    \begin{itemize}
        \item $n: 120 \to 240$: tiempo crece $\sim$3.9x
        \item $n: 240 \to 480$: tiempo crece $\sim$6.5x
        \item Variabilidad debido al número de iteraciones del subgradiente
    \end{itemize}
\end{itemize}

El comportamiento de DCMST se ajusta razonablemente a su complejidad teórica $O(n^3)$.
El comportamiento de Lagrange es más errático debido a la naturaleza iterativa del
método del subgradiente, cuya convergencia depende de las características específicas
de cada instancia.

\subsection*{Evaluación de Calidad de Soluciones}
\addcontentsline{toc}{subsection}{Evaluación de Calidad de Soluciones}

Para complementar el análisis de escalabilidad y tiempos de ejecución, se evaluó la \textbf{calidad de las soluciones} obtenidas por los tres algoritmos implementados sobre un conjunto representativo de instancias de prueba.  
Se registraron los siguientes indicadores:

\begin{itemize}
    \item \textbf{[OK] Correctas:} soluciones óptimas encontradas.
    \item \textbf{[FAIL] Incorrectas:} soluciones no óptimas.
    \item \textbf{[ERROR] Errores fatales:} fallos durante la ejecución.
    \item \textbf{[TIMEOUT] Timeout:} no se completó la ejecución dentro del límite de tiempo.
    \item \textbf{[MEMORY] Memory:}  no se completó la ejecución dentro del límite de memoria.
\end{itemize}

\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
\textbf{Algorithm} & \textbf{Result} & \textbf{[OK]} & \textbf{[FAIL]} & \textbf{[ERROR]} & \textbf{[TIMEOUT]} & \textbf{[MEMORY]} \\
\hline
Brute Force & [OK] & 100 & 0 & 0 & 0 & 0 \\
DCMST & [FAIL] & 95 & 5 & 0 & 0 & 0 \\
Lagrange & [OK] & 100 & 0 & 0 & 0 & 0 \\
\hline
\end{tabular}
\caption{Calidad de soluciones de los algoritmos implementados}
\label{tab:calidad_soluciones}
\end{table}

\paragraph{Interpretación:}  
\begin{itemize}
    \item El algoritmo \textbf{Brute Force} obtiene la solución óptima en todas las instancias evaluadas dentro del límite de tamaño considerado ($n \leq 15$), como era de esperar dada su naturaleza exhaustiva y el control sobre el tamaño de las instancias. Cabe destacar que, para grafos más grandes, la complejidad exponencial hace inviable su aplicación.  
    \item La heurística \textbf{DCMST} encuentra soluciones óptimas en un 95\% de los casos; en el 5\% restante, las restricciones de grado generan soluciones subóptimas o inviables, reflejando la naturaleza greedy de la aproximación.  
    \item El método \textbf{Lagrange} alcanza el 100\% de efectividad en las instancias evaluadas, demostrando su robustez al manejar restricciones y su capacidad de aproximación cercana al óptimo.
\end{itemize}

\paragraph{Conclusión:}  
Estos resultados muestran que, aunque DCMST es altamente eficiente en términos de escalabilidad y tiempo, puede fallar en un pequeño porcentaje de instancias. Por lo tanto, para garantizar factibilidad y óptimo en aplicaciones críticas, es recomendable combinar DCMST como primera aproximación con Lagrange para refinamiento de la solución.

\subsection*{Análisis de Comportamiento de las Heurísticas}
\addcontentsline{toc}{subsection}{Análisis de Comportamiento de las Heurísticas}

Esta sección presenta un análisis teórico de las características estructurales de las
instancias que, según el diseño de los algoritmos, deberían favorecer o perjudicar
su desempeño. Este análisis se basa en la lógica interna de cada heurística.

\subsubsection*{Características de Instancias Favorables (Análisis Teórico)}

\paragraph{Algoritmo DCMST - Casos Teóricamente Favorables:}

Basándose en su estrategia greedy, el algoritmo DCMST debería desempeñarse mejor en:

\begin{enumerate}
    \item \textbf{Restricciones uniformes:} Cuando todos los vértices tienen restricciones
    similares ($d(v) \approx \bar{d}$ para todo $v$), el algoritmo greedy no enfrenta
    saturaciones prematuras y puede seleccionar libremente las aristas más baratas.

    \item \textbf{Restricciones de grado relativamente altas:} Mientras mayores sean los valores
    de $d(v)$ y más alejados de 1, 2 o 3, el algoritmo tiene mayor libertad para seleccionar 
    aristas, reduciendo el riesgo de bloqueos prematuros y aumentando la probabilidad de encontrar 
    soluciones óptimas.

    \item \textbf{Costos bien distribuidos:} Si los costos siguen una distribución
    aproximadamente uniforme o normal, las aristas baratas no se concentran en
    vértices específicos, permitiendo al greedy construir soluciones balanceadas.

    \item \textbf{Grafos densos con alta conectividad:} En grafos completos o casi
    completos, existen múltiples alternativas de conexión, lo que reduce el impacto
    de decisiones greedy subóptimas.
\end{enumerate}

\paragraph{Algoritmo Lagrangiano - Casos Favorables:}

El algoritmo Lagrangiano encuentra el óptimo o soluciones muy cercanas cuando:

\begin{enumerate}
    \item \textbf{Restricciones moderadamente ajustadas:} Cuando $\sum d(v) \approx 2.5(n-1)$,
    el problema relajado captura bien la estructura del problema original y los
    multiplicadores convergen rápidamente.

    \item \textbf{Estructura de costos regular:} Cuando los costos no presentan
    outliers extremos (e.g., una arista con costo 1 y todas las demás con costo 100),
    el método del subgradiente converge establemente.

    \item \textbf{Grafos completos:} La garantía de factibilidad de KRUSKALX asegura
    que siempre se encuentra una solución, y la iteración del subgradiente la mejora
    progresivamente.

    \item \textbf{Simetría estructural:} En grafos con simetría (e.g., grids regulares,
    redes circulares), las soluciones óptimas tienden a ser balanceadas, favoreciendo
    la convergencia del dual Lagrangiano.
\end{enumerate}

\subsubsection*{¿Cuándo las Heurísticas NO Encuentran el Óptimo?}

\paragraph{Algoritmo DCMST - Casos Desfavorables:}

\begin{enumerate}
    \item \textbf{Restricciones extremadamente heterogéneas:} Cuando conviven vértices
    con $d(v)=1$ junto a vértices con $d(v) \gg n/2$, el algoritmo debe balancear
    prioridades contradictorias y las decisiones greedy frecuentemente son subóptimas.

    \item \textbf{Costos adversarios:} Si las $k$ aristas más baratas inciden todas
    en un mismo vértice $v$ con $d(v) \ll k$, el greedy solo puede usar $d(v)$ de
    ellas, desperdiciando oportunidades de bajo costo.

    \item \textbf{Grafos dispersos:} En grafos con $m \approx 2n$ (apenas más aristas
    que el mínimo necesario), las opciones son limitadas y el greedy puede tomar
    decisiones irreversibles que impiden completar el árbol.

    \item \textbf{Dependencias de orden:} Cuando el orden óptimo de adición de aristas
    requiere añadir temporalmente aristas más caras para reservar capacidad en
    vértices críticos, el greedy por costo creciente falla.
\end{enumerate}

\paragraph{Algoritmo Lagrangiano - Casos Desfavorables:}

\begin{enumerate}
    \item \textbf{Restricciones muy holgadas:} Cuando $\sum d(v) \gg 2(n-1)$, el
    problema relajado se aproxima demasiado al MST clásico y los multiplicadores
    no proporcionan información útil para guiar hacia el DC-MST óptimo.

    \item \textbf{Convergencia lenta del subgradiente:} En instancias con estructura
    degenerada (e.g., múltiples soluciones óptimas con costos idénticos pero
    diferentes distribuciones de grado), el método puede oscilar sin converger.

    \item \textbf{Número limitado de iteraciones:} Si se impone un límite estricto
    de iteraciones para mantener eficiencia, el algoritmo puede detenerse antes
    de alcanzar el gap $< 1$ necesario para certificar optimalidad.
\end{enumerate}

\subsubsection*{Caso de Estudio: Fallo de Factibilidad en DCMST}

La siguiente instancia ilustra un caso extremo donde DCMST falla en encontrar cualquier
solución factible, a pesar de que existe:

\begin{lstlisting}
10 18
4 7 3
8 7 19
3 1 19
4 1 13
6 5 5
9 1 2
3 0 9
2 6 4
0 2 5
5 2 11
8 0 11
6 8 6
2 7 8
7 5 15
3 7 17
2 1 1
1 5 3
7 0 7
1 2 2 3 3 1 3 3 3 2
\end{lstlisting}

\paragraph{Análisis del Fallo:}

En esta instancia:
\begin{itemize}
    \item Varios vértices tienen restricciones muy ajustadas: $d \in \{1, 2, 3\}$
    \item El algoritmo greedy selecciona aristas baratas (9-1: 2, 2-1: 1, 1-5: 3)
          que saturan prematuramente los vértices 1 y 2
    \item Al intentar conectar las componentes restantes, descubre que los únicos
          puentes disponibles requieren usar vértices ya saturados
    \item Las técnicas de intercambio (1-opt, 2-opt) no se pueden aplicar porque
          no se ha construido un árbol generador completo
\end{itemize}

\textbf{Solución con Lagrange:} En contraste, el algoritmo Lagrangiano, gracias a su
condición de no saturación en KRUSKALX, está diseñado teóricamente para prevenir
saturaciones prematuras de componentes en grafos completos, lo que sugiere que podría
manejar este tipo de instancias más robustamente.

\subsubsection*{Resumen Comparativo: DCMST vs Lagrange}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Característica} & \textbf{DCMST} & \textbf{Lagrange} \\
\hline
\hline
Complejidad teórica & $O(n^3)$ & $O(K \cdot n^3 \log n)$ \\
\hline
Escalabilidad observada & $n \approx 960$ & $n \approx 480$ \\
\hline
Garantía de factibilidad (teórica) & No & Sí (grafos completos) \\
\hline
Proporciona cotas de calidad & No & Sí ($z_{LB}$) \\
\hline
Velocidad (n=240) & 170.35 ms & 132.81 ms \\
\hline
Velocidad (n=480) & 435.05 ms & 862.33 ms \\
\hline
\end{tabular}
\caption{Comparación de características de las heurísticas (basado en datos experimentales de escalabilidad)}
\end{table}

\subsubsection*{Recomendaciones de Uso}

Basándose en el análisis experimental:

\begin{itemize}
    \item \textbf{Instancias pequeñas ($n \leq 15$):} Usar \textbf{Fuerza Bruta} para
    obtener la solución óptima garantizada.

    \item \textbf{Instancias medianas ($15 < n \leq 500$) con restricciones ajustadas:}
    Usar \textbf{Lagrange} para garantizar factibilidad y buena calidad.

    \item \textbf{Instancias grandes ($n > 500$):} Usar \textbf{DCMST} para maximizar
    escalabilidad. Si falla, recurrir a Lagrange o implementar backtracking en el greedy.

    \item \textbf{Instancias con restricciones heterogéneas extremas:} Preferir
    \textbf{DCMST} si las restricciones de grado conocemos que suelen ser heterogéneas, 
    puedes existir algunas con $d(v)=2$ y otras con $d(v) \gg n/2$.

    \item \textbf{Grafos densos:} Preferir \textbf{Lagrange} para aprovechar la garantía de factibilidad y preferir
    \textbf{DCMST} donde se requiere mayor velocidad.

    \item \textbf{Aplicaciones de producción:} Combinar DCMST como primera aproximación
    rápida, seguido de Lagrange para refinamiento si se requiere mayor calidad.

\end{itemize}
\end{document}

